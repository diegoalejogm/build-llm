{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0525c683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year 1122\n",
      "years 1123\n",
      "yellow 1124\n",
      "yet 1125\n",
      "you 1126\n",
      "younger 1127\n",
      "your 1128\n",
      "yourself 1129\n",
      "<|endoftext|> 1130\n",
      "<|unk|> 1131\n",
      "[999, 1131, 1131, 1130, 584, 0, 0, 1077, 6]\n",
      "this <|unk|> <|unk|> <|endoftext|> is!! was--\n",
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  198, 11274,  5891,  1576],\n",
      "        [  438,   568,   340,   373],\n",
      "        [  645,  1049,  5975,   284],\n",
      "        [  502,   284,  3285,   326]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   198],\n",
      "        [11274,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "torch.Size([8, 4, 256])\n",
      "torch.Size([4, 256])\n",
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "%run data-processing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b21a195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      " Sov Cannon separat restaurant\n",
      " Sov targeting inadvert509\n",
      "Mean:\n",
      "  tensor([-0.3596, -0.2606])\n",
      "Variance :\n",
      "  tensor([0.2015, 0.2673])\n",
      "Norm. Mean:\n",
      "  tensor([    -0.0000,      0.0000], grad_fn=<MeanBackward1>)\n",
      "Norm. Variance :\n",
      "  tensor([1.0000, 1.0000], grad_fn=<VarBackward0>)\n",
      "tensor([[0.2685, 0.7413],\n",
      "        [0.2738, 0.7564],\n",
      "        [0.2668, 0.7366],\n",
      "        [0.2618, 0.7218],\n",
      "        [0.2712, 0.7495]], grad_fn=<MmBackward0>)\n",
      "tensor([[-0.4927, -0.0791],\n",
      "        [-0.4938, -0.0806],\n",
      "        [-0.4924, -0.0851],\n",
      "        [-0.4923, -0.0819],\n",
      "        [-0.4928, -0.0853]], grad_fn=<MmBackward0>)\n",
      "Self Attention V2 output: \n",
      " tensor([[-0.4927, -0.0791],\n",
      "        [-0.4938, -0.0806],\n",
      "        [-0.4924, -0.0851],\n",
      "        [-0.4923, -0.0819],\n",
      "        [-0.4928, -0.0853]], grad_fn=<MmBackward0>)\n",
      "Self Attention V1 output: \n",
      " tensor([[-0.4927, -0.0791],\n",
      "        [-0.4938, -0.0806],\n",
      "        [-0.4924, -0.0851],\n",
      "        [-0.4923, -0.0819],\n",
      "        [-0.4928, -0.0853]], grad_fn=<MmBackward0>)\n",
      "batch.shape: torch.Size([2, 5, 3])\n",
      "contextVecs.shape: torch.Size([2, 5, 2])\n",
      "batch.shape: torch.Size([2, 5, 3])\n",
      "contextVecs.shape: torch.Size([2, 5, 4])\n",
      "batch.shape: torch.Size([2, 5, 3])\n",
      "contextVecs.shape: torch.Size([2, 5, 2])\n",
      "batch.shape: torch.Size([2, 1024, 768])\n",
      "contextVecs.shape: torch.Size([2, 1024, 768])\n",
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n",
      "Output: tensor([[15496,    11,   314,   716, 24111, 43446, 11311, 42014, 15660, 43819]])\n",
      "Output length: 10\n",
      "Hello, I amFrench rebirth chocolate horny incentiveliterally\n"
     ]
    }
   ],
   "source": [
    "# Load GPT model\n",
    "%run gpt-model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9e9169",
   "metadata": {},
   "source": [
    "# Generating Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51abd887",
   "metadata": {},
   "source": [
    "Define a functionality that allows encoding and decoding text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d8aee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "def textToTokenIds(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    return torch.tensor(encoded).unsqueeze(0)\n",
    "\n",
    "def tokenIdsToText(tokens, tokenizer):\n",
    "    formatted = tokens.squeeze(0).tolist()\n",
    "    return tokenizer.decode(formatted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1ec210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello world <|endoftext|>\n",
      "Encoded:  tensor([[15496,   995,   220, 50256]])\n",
      "Decoded:  Hello world <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Test functionality to encode and decode text.\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "testText = \"Hello world <|endoftext|>\"\n",
    "print(\"Original:\", testText)\n",
    "\n",
    "encoded = textToTokenIds(testText, tokenizer)\n",
    "print(\"Encoded: \", encoded)\n",
    "\n",
    "decoded = tokenIdsToText(encoded, tokenizer)\n",
    "print(\"Decoded: \", decoded)\n",
    "\n",
    "\n",
    "del tokenizer, testText, encoded, decoded\n",
    "# generateText(model, textToTokenIds(testText, tokenizer), 10, cfg[\"context_size\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2fbdb1",
   "metadata": {},
   "source": [
    "Test functionality to run GPT model to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7092dbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded output:\n",
      "  tensor([[15496,   995, 27018, 48553, 41946, 10752,  3509, 37789, 34174, 41971,\n",
      "          6936,  1246]])\n",
      "Encoded generated text:\n",
      "  Hello world Featurewrapper Medina coin max mailedarat Bauer armed contro\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "maxNewTokens = 10\n",
    "testText = \"Hello world\"\n",
    "\n",
    "# Run model\n",
    "encodedOutput = generateText(model, textToTokenIds(testText, tokenizer), max_new_tokens, cfg[\"context_length\"] )\n",
    "print(\"Encoded output:\\n \", encodedOutput)\n",
    "\n",
    "print(\"Encoded generated text:\\n \", tokenIdsToText(encodedOutput, tokenizer))\n",
    "\n",
    "del tokenizer, max_new_tokens, encodedOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292ced25",
   "metadata": {},
   "source": [
    "## Calculating the text generation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c445754a",
   "metadata": {},
   "source": [
    "The loss is needed to backpropagate the mistakes.\n",
    "\n",
    "We can use the cross entropy loss, which tries to minimize the difference between two distributions: the model output distribution and the input text distribution (i.e. for evaluation or training). \n",
    "\n",
    "To do this, we first transform the output logits of the model into a probability distribution, using the softmax function. Then, we compute the cross entropy loss.\n",
    "\n",
    "#### Cross Entropy\n",
    "\n",
    "The cross entropy is defined as the negative log likelihood of the probability. \n",
    "- The closer to `1` the predicted probability is, the lower the loss. \n",
    "- Likewise, the predicted ouptut closer to `0`, the higher the loss.\n",
    "\n",
    "We provide batches of inputs to the model, thus we will compute the average cross entropy loss per batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477d025d",
   "metadata": {},
   "source": [
    "#### Manual implementation\n",
    "First of all, we will implement Cross Entropy manually, to better understand the concept.\n",
    "\n",
    "1. Get logits by running model.\n",
    "2. Compute probabilities as `softmax(logits)`.\n",
    "3. Get `target_probabilities` corresponding to the `target_ids`.\n",
    "4. Compute Cross Entropy as `-log(target_probabilities)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0935204a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "  ['every effort moves', 'I really like']\n",
      "Prediction:\n",
      "  [' ultrasDERRGy', 'antespect 157']\n",
      "Target batch:\n",
      "  tensor([[16833,  3626,  6100],\n",
      "        [   40,  1107,   588]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "maxNewTokens = 10\n",
    "inputs = [\"every effort moves\", \"I really like\"]\n",
    "targets = [\"effort moves you\", \"really like chocolate\"]\n",
    "\n",
    "print(\"Inputs:\\n \", inputs)\n",
    "\n",
    "## Generate input logit tensors\n",
    "input_list = [textToTokenIds(i, tokenizer) for i in inputs]\n",
    "input_batch = torch.cat(input_list, dim=0)\n",
    "logits = model(input_batch)\n",
    "prediction = torch.argmax(logits, dim=-1)\n",
    "prediction_txt = [tokenIdsToText(p, tokenizer) for p in prediction]\n",
    "print(\"Prediction:\\n \", prediction_txt)\n",
    "\n",
    "## Generate target tensors\n",
    "target_list = [textToTokenIds(i, tokenizer) for i in inputs]\n",
    "target_batch = torch.cat(target_list, dim=0)\n",
    "print(\"Target batch:\\n \", target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73c25c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:\n",
      "  tensor([[[-0.9288,  1.0849, -0.5108,  ...,  0.8052, -0.3395, -0.1025],\n",
      "         [ 0.3700,  0.1617, -0.2742,  ...,  1.3756,  0.6312, -0.8306],\n",
      "         [ 0.1450, -0.7730, -0.2495,  ...,  1.8172, -0.3540, -0.1990]],\n",
      "\n",
      "        [[-0.5140, -0.1732,  0.0701,  ...,  1.1648,  0.0267, -0.7968],\n",
      "         [-0.3210, -0.2743, -0.4206,  ...,  1.4472,  1.1171, -0.9772],\n",
      "         [-0.0478, -1.0074,  0.1645,  ...,  1.5331, -0.4145,  0.2745]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "Probas:\n",
      "  tensor([[[    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0001,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0001,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        [[    0.0000,     0.0000,     0.0000,  ...,     0.0001,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0001,\n",
      "              0.0001,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0001,\n",
      "              0.0000,     0.0000]]], grad_fn=<SoftmaxBackward0>)\n",
      "Targer Probas:\n",
      "  tensor([[    0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000]], grad_fn=<SqueezeBackward1>)\n",
      "Log Probas:\n",
      "  tensor([-10.3653, -11.9684, -10.5609, -10.7176, -10.5513, -10.5631],\n",
      "       grad_fn=<LogBackward0>)\n",
      "Avg. log. Probas:\n",
      "  tensor([-10.3653, -11.9684, -10.5609, -10.7176, -10.5513, -10.5631],\n",
      "       grad_fn=<LogBackward0>)\n",
      "Cross Entropy Loss:\n",
      "  tensor(10.7878, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Fetch probabilities for target tensors\n",
    "print(\"Logits:\\n \", logits)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(\"Probas:\\n \", probas)\n",
    "target_probas = torch.gather(probas, -1, target_batch.unsqueeze(-1)).squeeze(-1)\n",
    "print(\"Targer Probas:\\n \", target_probas)\n",
    "\n",
    "## Compute cross entropy\n",
    "log_probas = torch.log(target_probas.flatten())\n",
    "print(\"Log Probas:\\n \", log_probas)\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(\"Avg. log. Probas:\\n \", log_probas)\n",
    "neg_avg_log_probas = torch.mean(avg_log_probas) * -1\n",
    "print(\"Cross Entropy Loss:\\n \", neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29efb36",
   "metadata": {},
   "source": [
    "#### Compute using Torch Cross Entropy function\n",
    "\n",
    "Now that we've implemented the cross entropy manually, we can just use the torch available functions to confirm it was correctly implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42b96bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.7878, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0,1)\n",
    "targets_flat = target_batch.flatten()\n",
    "torch.nn.functional.cross_entropy(logits_flat, targets_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99ef423",
   "metadata": {},
   "source": [
    "## Training & Validation losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196561d0",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bf425c",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b353d13",
   "metadata": {},
   "source": [
    "##### Load text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48830396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5477\n"
     ]
    }
   ],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da36141f",
   "metadata": {},
   "source": [
    "##### Split train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13961777",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b6853",
   "metadata": {},
   "source": [
    "##### Create dataloaders using splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "edb66ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=cfg[\"context_length\"],\n",
    "    stride=cfg[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=cfg[\"context_length\"],\n",
    "    stride=cfg[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f34aa66",
   "metadata": {},
   "source": [
    "##### Display loader contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d56b4219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7024952",
   "metadata": {},
   "source": [
    "### Loss Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5f65f1",
   "metadata": {},
   "source": [
    "##### Batch loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c444ec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given batch, compute the cross entropy.\n",
    "def calc_loss_batch(input_batch: torch.Tensor, target_batch: torch.Tensor, model: torch.nn.Module, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    # Run model.\n",
    "    logits = model(input_batch)\n",
    "    # Compute cross-entropy loss.\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b165c99",
   "metadata": {},
   "source": [
    "##### Data loader loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2c41693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the loss for a given data_loader.\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=torch.inf):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    # Select num_batches to be at most len(data_loader)/\n",
    "    num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    # Iterate through batches and compute sum loss.\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        loss = calc_loss_batch(\n",
    "            input_batch, target_batch, model, device\n",
    "        )\n",
    "        total_loss += loss.item()\n",
    "    # Return avg. loss.\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff0c047",
   "metadata": {},
   "source": [
    "##### Compute initial model loss\n",
    "\n",
    "Run calc_loss_loader() on the train and eval sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12a373f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 11.025343153211805\n",
      "Validation loss: 10.95755672454834\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b46563",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7057f0",
   "metadata": {},
   "source": [
    "##### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d80ab840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the train and validation loaders. Runs only on the `eval_iter` number of batches.\n",
    "# Returns the train and validation losses.\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5011ca",
   "metadata": {},
   "source": [
    "##### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d539914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates text.\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context, temperature : int = 0.0, top_k=None, eos_id=None):\n",
    "    model.eval()\n",
    "    context_size = model.posEmbed.weight.shape[0]\n",
    "    encoded = textToTokenIds(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generateText(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size,\n",
    "            temperature=temperature, top_k=top_k\n",
    "        )\n",
    "    decoded_text = tokenIdsToText(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a8c39b",
   "metadata": {},
   "source": [
    "##### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81f2fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the model. \n",
    "# Returns the train, validation losses, and a list of all the tokens seen during training.\n",
    "def train_model_simple(model, train_loader, val_loader,\n",
    "                       optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    def print_evals():\n",
    "        train_loss, val_loss = evaluate_model(\n",
    "            model, train_loader, val_loader, device, eval_iter\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        track_tokens_seen.append(tokens_seen)\n",
    "        print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "            f\"Train loss {train_loss:.3f}, \"\n",
    "            f\"Val loss {val_loss:.3f}\"\n",
    "        )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(\n",
    "                    input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Print evals.\n",
    "            if global_step % eval_freq == 0:\n",
    "                print_evals()\n",
    "\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aa3a77",
   "metadata": {},
   "source": [
    "##### Load Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab273422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 256)\n",
       "  (pos_emb): Embedding(256, 256)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (W_query): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (W_value): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (W_query): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (W_value): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (W_query): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (W_value): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (W_query): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (W_value): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=256, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "cfg = {\n",
    "    \"vocabSize\": 50257, # Number of tokens in vocabulary.\n",
    "    \"context_length\": 256, # Max. number of tokens the LLM sees per run.\n",
    "    \"emb_dim\": 256, # Size of the internal. embeddings used in the LLM attention mechanism.\n",
    "    \"n_layers\" : 4, # Number of transformer layers.\n",
    "    \"dropRate\": 0.1, # Feature dropout rate.\n",
    "    \"n_heads\": 4, # Num. Attention heads per multi-head attention block    \n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "# cfg = {\n",
    "#     \"vocabSize\": 50257, # Number of tokens in vocabulary.\n",
    "#     \"context_length\": 256, # Max. number of tokens the LLM sees per run.\n",
    "#     \"emb_dim\": 768, # Size of the internal. embeddings used in the LLM attention mechanism.\n",
    "#     \"n_layers\" : 6, # Number of transformer layers.\n",
    "#     \"dropRate\": 0.1, # Feature dropout rate.\n",
    "#     \"n_heads\": 12 # Num. Attention heads per multi-head attention block    \n",
    "# }\n",
    "\n",
    "model = GPTModel(cfg)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076b3c7d",
   "metadata": {},
   "source": [
    "##### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9675e871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 19.553, Val loss 10.866\n",
      "Ep 1 (Step 000005): Train loss 17.871, Val loss 10.096\n",
      "Every effort moves you                                                  \n",
      "Ep 2 (Step 000010): Train loss 16.281, Val loss 9.278\n",
      "Ep 2 (Step 000015): Train loss 14.997, Val loss 8.618\n",
      "Every effort moves you                                                  \n",
      "Ep 3 (Step 000020): Train loss 13.742, Val loss 7.996\n",
      "Ep 3 (Step 000025): Train loss 12.721, Val loss 7.475\n",
      "Every effort moves you                                                  \n",
      "Ep 4 (Step 000030): Train loss 11.902, Val loss 7.118\n",
      "Ep 4 (Step 000035): Train loss 11.403, Val loss 6.908\n",
      "Every effort moves you                                                  \n",
      "Ep 5 (Step 000040): Train loss 11.083, Val loss 6.794\n",
      "Every effort moves you                                                  \n",
      "Ep 6 (Step 000045): Train loss 10.910, Val loss 6.725\n",
      "Ep 6 (Step 000050): Train loss 10.774, Val loss 6.697\n",
      "Every effort moves you                                                  \n",
      "Ep 7 (Step 000055): Train loss 10.706, Val loss 6.690\n",
      "Ep 7 (Step 000060): Train loss 10.526, Val loss 6.663\n",
      "Every effort moves you,                                                 \n",
      "Ep 8 (Step 000065): Train loss 10.305, Val loss 6.635\n",
      "Ep 8 (Step 000070): Train loss 10.114, Val loss 6.625\n",
      "Every effort moves you,                            \", and, the              \",\n",
      "Ep 9 (Step 000075): Train loss 9.913, Val loss 6.564\n",
      "Ep 9 (Step 000080): Train loss 9.732, Val loss 6.526\n",
      "Every effort moves you,          \", and the of the            \", and the               \",\n",
      "Ep 10 (Step 000085): Train loss 9.428, Val loss 6.521\n",
      "Every effort moves you,                       , the, and, and, and, and, and, and, and, the, and, and, and, and\n",
      "Ep 11 (Step 000090): Train loss 9.270, Val loss 6.510\n",
      "Ep 11 (Step 000095): Train loss 8.992, Val loss 6.492\n",
      "Every effort moves you,          \", the          \", and.                    \",\n",
      "Ep 12 (Step 000100): Train loss 8.743, Val loss 6.447\n",
      "Ep 12 (Step 000105): Train loss 8.543, Val loss 6.440\n",
      "Every effort moves you,   \" of the   \"I, and \"--the it.  \"--.  \"--and, and I had to    \"--    \"--the, and\n",
      "Ep 13 (Step 000110): Train loss 8.209, Val loss 6.401\n",
      "Ep 13 (Step 000115): Train loss 7.962, Val loss 6.389\n",
      "Every effort moves you,   \" of the   \"I, the \"--I of the  \"--. I \"--and, and I had been the   \"I    \"--and, and\n",
      "Ep 14 (Step 000120): Train loss 7.734, Val loss 6.369\n",
      "Ep 14 (Step 000125): Train loss 7.442, Val loss 6.367\n",
      "Every effort moves you, I  \" of the   \"I, the \"--and to the \"--the. I \"--and, and I \"I of the the of the   \"--the--as\n",
      "Ep 15 (Step 000130): Train loss 7.181, Val loss 6.347\n",
      "Every effort moves you, I   \" aburn. \"Oh, the I of the  \" of the \"--I of the   the it, I had to the, and. \"I, and, I \n",
      "Ep 16 (Step 000135): Train loss 6.915, Val loss 6.333\n",
      "Ep 16 (Step 000140): Train loss 6.643, Val loss 6.326\n",
      "Every effort moves you of   \"Oh, with a in a little, the I of the  \"--and of the he had been--and, and he was his_ to me, and     \"--and, and\n",
      "Ep 17 (Step 000145): Train loss 6.369, Val loss 6.338\n",
      "Ep 17 (Step 000150): Train loss 6.094, Val loss 6.337\n",
      "Every effort moves you of  \"I of the- it, to have- to the  \"Oh, and I was \"I of the--and, and in the \"--and.  \"I of the, and he was to\n",
      "Ep 18 (Step 000155): Train loss 5.828, Val loss 6.357\n",
      "Ep 18 (Step 000160): Train loss 5.667, Val loss 6.358\n",
      "Every effort moves you of  \"I, a, my of the Russian the I of the it was his pictures--the.  \"--and, I had been the_--and.  \"Oh, a, I had I \n",
      "Ep 19 (Step 000165): Train loss 5.335, Val loss 6.344\n",
      "Ep 19 (Step 000170): Train loss 5.032, Val loss 6.356\n",
      "Every effort moves you know I  \"Oh, with a in a little I was his own.  \"Oh,  \"Oh, and he was, I had been the_--and.  \"Oh, and He.  \n",
      "Ep 20 (Step 000175): Train loss 4.843, Val loss 6.402\n",
      "Every effort moves you know I had \"Oh, with  \"Oh, to the of the fact it was his pictures the-t a monumental--and, I had been the_--and, I was \"--the He showed it to\n",
      "Ep 21 (Step 000180): Train loss 4.586, Val loss 6.378\n",
      "Ep 21 (Step 000185): Train loss 4.303, Val loss 6.383\n",
      "Every effort moves you know I had \"Oh, with \"Oh, in the I of the couple at the \"Oh, I \"--and, you know. \"_--and to see \"Oh, with a little I was \n",
      "Ep 22 (Step 000190): Train loss 4.037, Val loss 6.408\n",
      "Ep 22 (Step 000195): Train loss 3.834, Val loss 6.398\n",
      "Every effort moves you  \", with a a little  \"Oh, to the prism of it to me--I felt the donkey it.  \"I didn't it took the light \"Oh, the house.\" \"I of the end\n",
      "Ep 23 (Step 000200): Train loss 3.607, Val loss 6.407\n",
      "Ep 23 (Step 000205): Train loss 3.408, Val loss 6.464\n",
      "Every effort moves you know I had the and in a dark in a pale to the I found the couple at tea beneath the-trees; and Mrs. the_ fashionable painter--and to have been \"Oh, with a, I \n",
      "Ep 24 (Step 000210): Train loss 3.194, Val loss 6.465\n",
      "Ep 24 (Step 000215): Train loss 2.913, Val loss 6.491\n",
      "Every effort moves you know I had the and \"--the loss I had  I found the couple at tea beneath  \"I didn't--and in the donkey, and I seemed to see a smile. \"I turned back to my dear\n",
      "Ep 25 (Step 000220): Train loss 2.748, Val loss 6.496\n",
      "Every effort moves you know; but I felt as a cheap genius--though a good-hum.  \"Oh, he had to have been the moment I was, as a little I seemed to see a smile behind his pictures \"Oh, I \n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0004, weight_decay=0.1\n",
    ")\n",
    "num_epochs = 25\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a2f0a3",
   "metadata": {},
   "source": [
    "##### Display Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b56112a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def _to_list(x):\n",
    "    if torch.is_tensor(x):\n",
    "        # avoids .numpy() so no NumPy dependency\n",
    "        return x.detach().cpu().tolist()\n",
    "    # handle lists/tuples already\n",
    "    return x\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(\n",
    "        epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(_to_list(epochs_tensor), _to_list(tokens_seen), _to_list(train_losses), _to_list(val_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e66834e",
   "metadata": {},
   "source": [
    "##### Generate new output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e9ec8e",
   "metadata": {},
   "source": [
    "### Save Model State"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a3bbd5",
   "metadata": {},
   "source": [
    "Store the model state (weights) and optimizer state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96e3b0ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m      4\u001b[0m     },\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_and_optimizer.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    },\n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "336c4f36",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_and_optimizer.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[43mdevice\u001b[49m)\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m GPTModel(cfg)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
    "model = GPTModel(cfg)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f5386d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_model_simple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train again loaded model for 1 step.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_losses, val_losses, tokens_seen \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_simple\u001b[49m(\n\u001b[1;32m      3\u001b[0m     model, train_loader, val_loader, optimizer, device,\n\u001b[1;32m      4\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, eval_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, eval_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m      5\u001b[0m     start_context\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvery effort moves you\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_model_simple' is not defined"
     ]
    }
   ],
   "source": [
    "# Train again loaded model for 1 step.\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    gpt, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=1, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a356c360",
   "metadata": {},
   "source": [
    "### Assign Weights from pre-trained GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a7da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12, \"file_name\": \"gpt2-small-124M.pth\"},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16, \"file_name\": \"gpt2-medium-355M.pth\"},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20, \"file_name\": \"gpt2-large-774M.pth\"},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25, \"file_name\": \"gpt2-xl-1558M.pth\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d299ebe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you closer… You are all here for us… Please take us to God!\" she cried.\n",
      "But they knew to hold the\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "model_name = \"gpt2-xl (1558M)\"\n",
    "file_name = model_configs[model_name][\"file_name\"]\n",
    "NEW_CONFIG = cfg.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024})\n",
    "NEW_CONFIG.update({\"qkv_bias\": True})\n",
    "\n",
    "url = f\"https://huggingface.co/rasbt/gpt2-from-scratch-pytorch/resolve/main/{file_name}\"\n",
    "\n",
    "if not os.path.exists(file_name):\n",
    "    urllib.request.urlretrieve(url, file_name)\n",
    "    print(f\"Downloaded to {file_name}\")\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.load_state_dict(torch.load(file_name, weights_only=True))\n",
    "gpt.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpt.to(device)\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generateText(\n",
    "    model=gpt,\n",
    "    idx=textToTokenIds(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", tokenIdsToText(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91de7aa",
   "metadata": {},
   "source": [
    "### Evaluate GPT 2 checkpoint loss on the Shakespeare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac53b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 6.783, Val loss: 3.540\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loss, val_loss = evaluate_model(\n",
    "    gpt, train_loader, val_loader, device, eval_iter=5\n",
    ")\n",
    "print(f\"Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
