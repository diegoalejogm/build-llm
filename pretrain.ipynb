{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0525c683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year 1122\n",
      "years 1123\n",
      "yellow 1124\n",
      "yet 1125\n",
      "you 1126\n",
      "younger 1127\n",
      "your 1128\n",
      "yourself 1129\n",
      "<|endoftext|> 1130\n",
      "<|unk|> 1131\n",
      "[999, 1131, 1131, 1130, 584, 0, 0, 1077, 6]\n",
      "this <|unk|> <|unk|> <|endoftext|> is!! was--\n",
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  198, 11274,  5891,  1576],\n",
      "        [  438,   568,   340,   373],\n",
      "        [  645,  1049,  5975,   284],\n",
      "        [  502,   284,  3285,   326]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   198],\n",
      "        [11274,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "torch.Size([8, 4, 256])\n",
      "torch.Size([4, 256])\n",
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "%run data-processing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4b21a195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "Tiï¿½ sellers (?,\n",
      " antique CHAR retiring Motors\n",
      "Mean:\n",
      "  tensor([-0.3596, -0.2606])\n",
      "Variance :\n",
      "  tensor([0.2015, 0.2673])\n",
      "Norm. Mean:\n",
      "  tensor([    -0.0000,      0.0000], grad_fn=<MeanBackward1>)\n",
      "Norm. Variance :\n",
      "  tensor([1.0000, 1.0000], grad_fn=<VarBackward0>)\n",
      "tensor([[0.2685, 0.7413],\n",
      "        [0.2738, 0.7564],\n",
      "        [0.2668, 0.7366],\n",
      "        [0.2618, 0.7218],\n",
      "        [0.2712, 0.7495]], grad_fn=<MmBackward0>)\n",
      "tensor([[-0.4927, -0.0791],\n",
      "        [-0.4938, -0.0806],\n",
      "        [-0.4924, -0.0851],\n",
      "        [-0.4923, -0.0819],\n",
      "        [-0.4928, -0.0853]], grad_fn=<MmBackward0>)\n",
      "Self Attention V2 output: \n",
      " tensor([[-0.4927, -0.0791],\n",
      "        [-0.4938, -0.0806],\n",
      "        [-0.4924, -0.0851],\n",
      "        [-0.4923, -0.0819],\n",
      "        [-0.4928, -0.0853]], grad_fn=<MmBackward0>)\n",
      "Self Attention V1 output: \n",
      " tensor([[-0.4927, -0.0791],\n",
      "        [-0.4938, -0.0806],\n",
      "        [-0.4924, -0.0851],\n",
      "        [-0.4923, -0.0819],\n",
      "        [-0.4928, -0.0853]], grad_fn=<MmBackward0>)\n",
      "batch.shape: torch.Size([2, 5, 3])\n",
      "contextVecs.shape: torch.Size([2, 5, 2])\n",
      "batch.shape: torch.Size([2, 5, 3])\n",
      "contextVecs.shape: torch.Size([2, 5, 4])\n",
      "batch.shape: torch.Size([2, 5, 3])\n",
      "contextVecs.shape: torch.Size([2, 5, 2])\n",
      "batch.shape: torch.Size([2, 1024, 768])\n",
      "contextVecs.shape: torch.Size([2, 1024, 768])\n",
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n",
      "Output: tensor([[15496,    11,   314,   716,  7778,  6165, 38367, 23688, 35137, 19494]])\n",
      "Output length: 10\n",
      "Hello, I am Coll ultimately Mum Spy shoved WHO\n"
     ]
    }
   ],
   "source": [
    "# Load GPT model\n",
    "%run gpt-model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9e9169",
   "metadata": {},
   "source": [
    "# Generating Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51abd887",
   "metadata": {},
   "source": [
    "Define a functionality that allows encoding and decoding text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9d8aee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "def textToTokenIds(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    return torch.tensor(encoded).unsqueeze(0)\n",
    "\n",
    "def tokenIdsToText(tokens, tokenizer):\n",
    "    formatted = tokens.squeeze(0).tolist()\n",
    "    return tokenizer.decode(formatted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7a1ec210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello world <|endoftext|>\n",
      "Encoded:  tensor([[15496,   995,   220, 50256]])\n",
      "Decoded:  Hello world <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Test functionality to encode and decode text.\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "testText = \"Hello world <|endoftext|>\"\n",
    "print(\"Original:\", testText)\n",
    "\n",
    "encoded = textToTokenIds(testText, tokenizer)\n",
    "print(\"Encoded: \", encoded)\n",
    "\n",
    "decoded = tokenIdsToText(encoded, tokenizer)\n",
    "print(\"Decoded: \", decoded)\n",
    "\n",
    "\n",
    "del tokenizer, testText, encoded, decoded\n",
    "# generateText(model, textToTokenIds(testText, tokenizer), 10, cfg[\"contextSize\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2fbdb1",
   "metadata": {},
   "source": [
    "Test functionality to run GPT model to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7092dbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded output:\n",
      "  tensor([[15496,   995, 16621, 37792,  9096, 29641, 33062, 32260, 37431,   815,\n",
      "         38790,  8833]])\n",
      "Encoded generated text:\n",
      "  Hello world expressing basilheim bots dungeons productions decon shouldjri occurs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "maxNewTokens = 10\n",
    "testText = \"Hello world\"\n",
    "\n",
    "# Run model\n",
    "encodedOutput = generateText(model, textToTokenIds(testText, tokenizer), maxNewTokens, cfg[\"contextLength\"] )\n",
    "print(\"Encoded output:\\n \", encodedOutput)\n",
    "\n",
    "print(\"Encoded generated text:\\n \", tokenIdsToText(encodedOutput, tokenizer))\n",
    "\n",
    "del tokenizer, maxNewTokens, encodedOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292ced25",
   "metadata": {},
   "source": [
    "## Calculating the text generation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c445754a",
   "metadata": {},
   "source": [
    "The loss is needed to backpropagate the mistakes.\n",
    "\n",
    "We can use the cross entropy loss, which tries to minimize the difference between two distributions: the model output distribution and the input text distribution (i.e. for evaluation or training). \n",
    "\n",
    "To do this, we first transform the output logits of the model into a probability distribution, using the softmax function. Then, we compute the cross entropy loss.\n",
    "\n",
    "#### Cross Entropy\n",
    "\n",
    "The cross entropy is defined as the negative log likelihood of the probability. \n",
    "- The closer to `1` the predicted probability is, the lower the loss. \n",
    "- Likewise, the predicted ouptut closer to `0`, the higher the loss.\n",
    "\n",
    "We provide batches of inputs to the model, thus we will compute the average cross entropy loss per batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477d025d",
   "metadata": {},
   "source": [
    "#### Manual implementation\n",
    "First of all, we will implement Cross Entropy manually, to better understand the concept.\n",
    "\n",
    "1. Get logits by running model.\n",
    "2. Compute probabilities as `softmax(logits)`.\n",
    "3. Get `target_probabilities` corresponding to the `target_ids`.\n",
    "4. Compute Cross Entropy as `-log(target_probabilities)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0935204a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "  ['every effort moves', 'I really like']\n",
      "Prediction:\n",
      "  [' holdings awaiting meme', ' Elsa pimisition']\n",
      "Target batch:\n",
      "  tensor([[16833,  3626,  6100],\n",
      "        [   40,  1107,   588]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "maxNewTokens = 10\n",
    "inputs = [\"every effort moves\", \"I really like\"]\n",
    "targets = [\"effort moves you\", \"really like chocolate\"]\n",
    "\n",
    "print(\"Inputs:\\n \", inputs)\n",
    "\n",
    "## Generate input logit tensors\n",
    "input_list = [textToTokenIds(i, tokenizer) for i in inputs]\n",
    "input_batch = torch.cat(input_list, dim=0)\n",
    "logits = model(input_batch)\n",
    "prediction = torch.argmax(logits, dim=-1)\n",
    "prediction_txt = [tokenIdsToText(p, tokenizer) for p in prediction]\n",
    "print(\"Prediction:\\n \", prediction_txt)\n",
    "\n",
    "## Generate target tensors\n",
    "target_list = [textToTokenIds(i, tokenizer) for i in inputs]\n",
    "target_batch = torch.cat(target_list, dim=0)\n",
    "print(\"Target batch:\\n \", target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "73c25c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:\n",
      "  tensor([[[    -0.2121,      0.6064,     -1.1729,  ...,     -0.2882,\n",
      "               0.0392,      0.4279],\n",
      "         [    -1.3114,      0.4351,     -0.7188,  ...,      0.0499,\n",
      "              -0.2677,      0.1474],\n",
      "         [    -0.9966,      0.0661,      0.3436,  ...,     -0.1621,\n",
      "              -0.0603,     -0.3128]],\n",
      "\n",
      "        [[    -0.2090,      0.1196,     -0.3624,  ...,     -0.7420,\n",
      "               0.1617,      0.1033],\n",
      "         [    -0.8233,     -0.0741,     -0.6599,  ...,     -0.7223,\n",
      "               0.4209,     -0.5346],\n",
      "         [    -0.2380,     -0.3344,     -0.3720,  ...,      0.8198,\n",
      "               0.4174,     -0.0000]]], grad_fn=<ViewBackward0>)\n",
      "Probas:\n",
      "  tensor([[[    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        [[    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000]]], grad_fn=<SoftmaxBackward0>)\n",
      "Targer Probas:\n",
      "  tensor([[    0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000]], grad_fn=<SqueezeBackward1>)\n",
      "Log Probas:\n",
      "  tensor([-11.1855, -11.6262, -11.0694, -11.7257, -10.0780, -10.9115],\n",
      "       grad_fn=<LogBackward0>)\n",
      "Avg. log. Probas:\n",
      "  tensor([-11.1855, -11.6262, -11.0694, -11.7257, -10.0780, -10.9115],\n",
      "       grad_fn=<LogBackward0>)\n",
      "Cross Entropy Loss:\n",
      "  tensor(11.0994, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Fetch probabilities for target tensors\n",
    "print(\"Logits:\\n \", logits)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(\"Probas:\\n \", probas)\n",
    "target_probas = torch.gather(probas, -1, target_batch.unsqueeze(-1)).squeeze(-1)\n",
    "print(\"Targer Probas:\\n \", target_probas)\n",
    "\n",
    "## Compute cross entropy\n",
    "log_probas = torch.log(target_probas.flatten())\n",
    "print(\"Log Probas:\\n \", log_probas)\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(\"Avg. log. Probas:\\n \", log_probas)\n",
    "neg_avg_log_probas = torch.mean(avg_log_probas) * -1\n",
    "print(\"Cross Entropy Loss:\\n \", neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29efb36",
   "metadata": {},
   "source": [
    "#### Compute using Torch Cross Entropy function\n",
    "\n",
    "Now that we've implemented the cross entropy manually, we can just use the torch available functions to confirm it was correctly implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "42b96bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.0994, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0,1)\n",
    "targets_flat = target_batch.flatten()\n",
    "torch.nn.functional.cross_entropy(logits_flat, targets_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99ef423",
   "metadata": {},
   "source": [
    "## Training & Validation losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "48830396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5477\n"
     ]
    }
   ],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "13961777",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "edb66ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(\n",
    "train_data,\n",
    "batch_size=2,\n",
    "max_length=cfg[\"contextLength\"],\n",
    "stride=cfg[\"contextLength\"],\n",
    "drop_last=True,\n",
    "shuffle=True,\n",
    "num_workers=0\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "val_data,\n",
    "batch_size=2,\n",
    "max_length=cfg[\"contextLength\"],\n",
    "stride=cfg[\"contextLength\"],\n",
    "drop_last=False,\n",
    "shuffle=False,\n",
    "num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b73810de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
      "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
      "\n",
      "Validation loader:\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c444ec20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
