{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b21a195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diego/Documents/Projects/build-llm/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      " Att Trader willpower Carn\n",
      " Tokyo uber apartments targets\n",
      "Mean:\n",
      "  tensor([-0.3596, -0.2606])\n",
      "Variance :\n",
      "  tensor([0.2015, 0.2673])\n",
      "Norm. Mean:\n",
      "  tensor([    -0.0000,      0.0000], grad_fn=<MeanBackward1>)\n",
      "Norm. Variance :\n",
      "  tensor([1.0000, 1.0000], grad_fn=<VarBackward0>)\n",
      "tensor([[0.2685, 0.7413],\n",
      "        [0.2738, 0.7564],\n",
      "        [0.2668, 0.7366],\n",
      "        [0.2618, 0.7218],\n",
      "        [0.2712, 0.7495]], grad_fn=<MmBackward0>)\n",
      "tensor([[-0.4927, -0.0791],\n",
      "        [-0.4938, -0.0806],\n",
      "        [-0.4924, -0.0851],\n",
      "        [-0.4923, -0.0819],\n",
      "        [-0.4928, -0.0853]], grad_fn=<MmBackward0>)\n",
      "Self Attention V2 output: \n",
      " tensor([[-0.4927, -0.0791],\n",
      "        [-0.4938, -0.0806],\n",
      "        [-0.4924, -0.0851],\n",
      "        [-0.4923, -0.0819],\n",
      "        [-0.4928, -0.0853]], grad_fn=<MmBackward0>)\n",
      "Self Attention V1 output: \n",
      " tensor([[-0.4927, -0.0791],\n",
      "        [-0.4938, -0.0806],\n",
      "        [-0.4924, -0.0851],\n",
      "        [-0.4923, -0.0819],\n",
      "        [-0.4928, -0.0853]], grad_fn=<MmBackward0>)\n",
      "batch.shape: torch.Size([2, 5, 3])\n",
      "contextVecs.shape: torch.Size([2, 5, 2])\n",
      "batch.shape: torch.Size([2, 5, 3])\n",
      "contextVecs.shape: torch.Size([2, 5, 4])\n",
      "batch.shape: torch.Size([2, 5, 3])\n",
      "contextVecs.shape: torch.Size([2, 5, 2])\n",
      "batch.shape: torch.Size([2, 1024, 768])\n",
      "contextVecs.shape: torch.Size([2, 1024, 768])\n",
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n",
      "Output: tensor([[15496,    11,   314,   716,  7778,  6165, 38367, 23688, 35137, 19494]])\n",
      "Output length: 10\n",
      "Hello, I am Coll ultimately Mum Spy shoved WHO\n"
     ]
    }
   ],
   "source": [
    "# Load GPT model\n",
    "%run gpt-model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9e9169",
   "metadata": {},
   "source": [
    "# Generating Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51abd887",
   "metadata": {},
   "source": [
    "Define a functionality that allows encoding and decoding text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d8aee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "def textToTokenIds(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    return torch.tensor(encoded).unsqueeze(0)\n",
    "\n",
    "def tokenIdsToText(tokens, tokenizer):\n",
    "    formatted = tokens.squeeze(0).tolist()\n",
    "    return tokenizer.decode(formatted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a1ec210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello world <|endoftext|>\n",
      "Encoded:  tensor([[15496,   995,   220, 50256]])\n",
      "Decoded:  Hello world <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Test functionality to encode and decode text.\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "testText = \"Hello world <|endoftext|>\"\n",
    "print(\"Original:\", testText)\n",
    "\n",
    "encoded = textToTokenIds(testText, tokenizer)\n",
    "print(\"Encoded: \", encoded)\n",
    "\n",
    "decoded = tokenIdsToText(encoded, tokenizer)\n",
    "print(\"Decoded: \", decoded)\n",
    "\n",
    "\n",
    "del tokenizer, testText, encoded, decoded\n",
    "# generateText(model, textToTokenIds(testText, tokenizer), 10, cfg[\"contextSize\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2fbdb1",
   "metadata": {},
   "source": [
    "Test functionality to run GPT model to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7092dbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded output:\n",
      "  tensor([[15496,   995, 16621, 37792,  9096, 29641, 33062, 32260, 37431,   815,\n",
      "         38790,  8833]])\n",
      "Encoded generated text:\n",
      "  Hello world expressing basilheim bots dungeons productions decon shouldjri occurs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "maxNewTokens = 10\n",
    "testText = \"Hello world\"\n",
    "\n",
    "# Run model\n",
    "encodedOutput = generateText(model, textToTokenIds(testText, tokenizer), maxNewTokens, cfg[\"contextLength\"] )\n",
    "print(\"Encoded output:\\n \", encodedOutput)\n",
    "\n",
    "print(\"Encoded generated text:\\n \", tokenIdsToText(encodedOutput, tokenizer))\n",
    "\n",
    "del tokenizer, maxNewTokens, encodedOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292ced25",
   "metadata": {},
   "source": [
    "## Calculating the text generation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c445754a",
   "metadata": {},
   "source": [
    "The loss is needed to backpropagate the mistakes.\n",
    "\n",
    "We can use the cross entropy loss, which tries to minimize the difference between two distributions: the model output distribution and the input text distribution (i.e. for evaluation or training). \n",
    "\n",
    "To do this, we first transform the output logits of the model into a probability distribution, using the softmax function. Then, we compute the cross entropy loss.\n",
    "\n",
    "#### Cross Entropy\n",
    "\n",
    "The cross entropy is defined as the negative log likelihood of the probability. \n",
    "- The closer to `1` the predicted probability is, the lower the loss. \n",
    "- Likewise, the predicted ouptut closer to `0`, the higher the loss.\n",
    "\n",
    "We provide batches of inputs to the model, thus we will compute the average cross entropy loss per batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477d025d",
   "metadata": {},
   "source": [
    "#### Manual implementation\n",
    "First of all, we will implement Cross Entropy manually, to better understand the concept.\n",
    "\n",
    "1. Get logits by running model.\n",
    "2. Compute probabilities as `softmax(logits)`.\n",
    "3. Get `target_probabilities` corresponding to the `target_ids`.\n",
    "4. Compute Cross Entropy as `-log(target_probabilities)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0935204a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "  ['every effort moves', 'I really like']\n",
      "Prediction:\n",
      "  [' holdings awaiting meme', ' Elsa pimisition']\n",
      "Target batch:\n",
      "  tensor([[16833,  3626,  6100],\n",
      "        [   40,  1107,   588]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "maxNewTokens = 10\n",
    "inputs = [\"every effort moves\", \"I really like\"]\n",
    "targets = [\"effort moves you\", \"really like chocolate\"]\n",
    "\n",
    "print(\"Inputs:\\n \", inputs)\n",
    "\n",
    "## Generate input logit tensors\n",
    "input_list = [textToTokenIds(i, tokenizer) for i in inputs]\n",
    "input_batch = torch.cat(input_list, dim=0)\n",
    "logits = model(input_batch)\n",
    "prediction = torch.argmax(logits, dim=-1)\n",
    "prediction_txt = [tokenIdsToText(p, tokenizer) for p in prediction]\n",
    "print(\"Prediction:\\n \", prediction_txt)\n",
    "\n",
    "## Generate target tensors\n",
    "target_list = [textToTokenIds(i, tokenizer) for i in inputs]\n",
    "target_batch = torch.cat(target_list, dim=0)\n",
    "print(\"Target batch:\\n \", target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "73c25c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:\n",
      "  tensor([[[    -0.2121,      0.6064,     -1.1729,  ...,     -0.2882,\n",
      "               0.0392,      0.4279],\n",
      "         [    -1.3114,      0.4351,     -0.7188,  ...,      0.0499,\n",
      "              -0.2677,      0.1474],\n",
      "         [    -0.9966,      0.0661,      0.3436,  ...,     -0.1621,\n",
      "              -0.0603,     -0.3128]],\n",
      "\n",
      "        [[    -0.2090,      0.1196,     -0.3624,  ...,     -0.7420,\n",
      "               0.1617,      0.1033],\n",
      "         [    -0.8233,     -0.0741,     -0.6599,  ...,     -0.7223,\n",
      "               0.4209,     -0.5346],\n",
      "         [    -0.2380,     -0.3344,     -0.3720,  ...,      0.8198,\n",
      "               0.4174,     -0.0000]]], grad_fn=<ViewBackward0>)\n",
      "Probas:\n",
      "  tensor([[[    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        [[    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000]]], grad_fn=<SoftmaxBackward0>)\n",
      "Targer Probas:\n",
      "  tensor([[    0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000]], grad_fn=<SqueezeBackward1>)\n",
      "Log Probas:\n",
      "  tensor([-11.1855, -11.6262, -11.0694, -11.7257, -10.0780, -10.9115],\n",
      "       grad_fn=<LogBackward0>)\n",
      "Avg. log. Probas:\n",
      "  tensor([-11.1855, -11.6262, -11.0694, -11.7257, -10.0780, -10.9115],\n",
      "       grad_fn=<LogBackward0>)\n",
      "Cross Entropy Loss:\n",
      "  tensor(11.0994, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Fetch probabilities for target tensors\n",
    "print(\"Logits:\\n \", logits)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(\"Probas:\\n \", probas)\n",
    "target_probas = torch.gather(probas, -1, target_batch.unsqueeze(-1)).squeeze(-1)\n",
    "print(\"Targer Probas:\\n \", target_probas)\n",
    "\n",
    "## Compute cross entropy\n",
    "log_probas = torch.log(target_probas.flatten())\n",
    "print(\"Log Probas:\\n \", log_probas)\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(\"Avg. log. Probas:\\n \", log_probas)\n",
    "neg_avg_log_probas = torch.mean(avg_log_probas) * -1\n",
    "print(\"Cross Entropy Loss:\\n \", neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29efb36",
   "metadata": {},
   "source": [
    "#### Compute using Torch Cross Entropy function\n",
    "\n",
    "Now that we've implemented the cross entropy manually, we can just use the torch available functions to confirm it was correctly implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "42b96bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.0994, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0,1)\n",
    "targets_flat = target_batch.flatten()\n",
    "torch.nn.functional.cross_entropy(logits_flat, targets_flat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
