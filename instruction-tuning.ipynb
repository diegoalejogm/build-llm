{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb8ef53b",
   "metadata": {},
   "source": [
    "# Instruction Tuning the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47868b1",
   "metadata": {},
   "source": [
    "### Download Instruction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233dcce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88b8190",
   "metadata": {},
   "source": [
    "Let's print one of the entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e237bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example entry:\\n\", data[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc93b88",
   "metadata": {},
   "source": [
    "##### TODO: Exercise 7.1 Changing prompt styles\n",
    "After fine-tuning the model with the Alpaca prompt style, try the Phi-3 prompt style\n",
    "shown in figure 7.4 and observe whether it affects the response quality of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f53227a",
   "metadata": {},
   "source": [
    "##### Implementing the prompt formatting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03847f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "    input = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    return instruction + input\n",
    "\n",
    "\n",
    "def format_output(entry):\n",
    "    return f\"\\n\\n### Response:\\n{entry['output']}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085e3c6e",
   "metadata": {},
   "source": [
    "Let's thest the new function prompt formatting function on a dataset entry, and confirm its output is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f2ed346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "test_data = data[50]\n",
    "print(format_input(test_data) + format_output(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a873059",
   "metadata": {},
   "source": [
    "Let's thest the new function prompt formatting function on an entry that's missin the input field, and confirm it output is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a1c14bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert 45 kilometers to meters.\n",
      "\n",
      "### Response:\n",
      "45 kilometers is 45000 meters.\n"
     ]
    }
   ],
   "source": [
    "test_data = data[2]\n",
    "print(format_input(test_data) + format_output(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51ae0e5",
   "metadata": {},
   "source": [
    "##### Partitioning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f43cd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.10)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion : train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion :]\n",
    "\n",
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3550c6d",
   "metadata": {},
   "source": [
    "### Organize Data into Training Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67a24c0",
   "metadata": {},
   "source": [
    "Efficiently padding the data samples to equal lenghts in order to assemble multiple instruction examples in a batch.\n",
    "\n",
    "\n",
    "1. **Format data using prompt template**.\n",
    "2. **Tokenize formatted data**.\n",
    "3. **Adjust inputs to same lenght** with padding tokens.\n",
    "4. **Create target token IDs**, by shifting the inputs by 1.\n",
    "5. **Replace padding tokens with placeholders**, to exclude them from training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b57829",
   "metadata": {},
   "source": [
    "##### Define Instruction Dataset\n",
    "That applies formatting and pretokenizes all inputs in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "916bf0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.encoded_texts = []\n",
    "\n",
    "        for entry in data:\n",
    "            formatted_input = format_input(entry)\n",
    "            formatted_output = format_output(entry)\n",
    "            full_text = formatted_input + formatted_output\n",
    "\n",
    "            self.encoded_texts.append(full_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7fdfdd",
   "metadata": {},
   "source": [
    "##### Tokenization\n",
    "\n",
    "Similar to the pre-training and classification fine-tuning, we use gpt tokenizer to encode the data, allowing for a special `eot` character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51da319b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f15cc5",
   "metadata": {},
   "source": [
    "##### Adjust inputs to the same lenght\n",
    "Define a collate function that adjusts inputs to the same lenght.\n",
    "\n",
    "A final version of the collate function will be used later on in the Dataloader to collate inputs into batches of the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d0cc23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_1(batch, pad_token_id=50256, device=\"cpu\"):\n",
    "    max_length = max(len(item) + 1 for item in batch)\n",
    "    inputs_lst = []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        padded = new_item + [pad_token_id] * (max_length - len(new_item))\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_lst.append(inputs)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    return inputs_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7488403b",
   "metadata": {},
   "source": [
    "Let's test it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb409980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "batch = (inputs_1, inputs_2, inputs_3)\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5356bb6e",
   "metadata": {},
   "source": [
    "As expected, the input with the longest length has no padding, whereas the other two inputs are padded to match the length of the first input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95beac5",
   "metadata": {},
   "source": [
    "##### Create Targets\n",
    "\n",
    "Let's update the collate function so we also compute the target tensors.\n",
    "\n",
    "The target tensors are the same as the input ones, shifted by one position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71d9da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_2(batch, pad_token_id=50256, device=\"cpu\"):\n",
    "    max_length = max(len(item) + 1 for item in batch)\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        padded = new_item + [pad_token_id] * (max_length - len(new_item))\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "110eaf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " : tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "Targets:\n",
      " : tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "batch = (inputs_1, inputs_2, inputs_3)\n",
    "print(f\"Inputs:\\n : {custom_collate_draft_2(batch)[0]}\")\n",
    "print(f\"Targets:\\n : {custom_collate_draft_2(batch)[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a100d7",
   "metadata": {},
   "source": [
    "##### Replace padding tokens with placeholders\n",
    "\n",
    "This allows us to exclude all padding tokens from contributing to the loss calculation.\n",
    "\n",
    "- Retaining the `eot` token allows the LLM to learn when to generate an end- of-text token in response to instructions, which we use as an indicator that the generated response is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bd237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch, pad_token_id=50256, ignore_index=-100, device=\"cpu\", allowed_max_length=None\n",
    "):\n",
    "    max_length = max(len(item) + 1 for item in batch)\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        padded = new_item + [pad_token_id] * (max_length - len(new_item))\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        # Replace extra padding tokens in targets by `ignore_index`.\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b74d26",
   "metadata": {},
   "source": [
    "Let's test it now. \n",
    "\n",
    "We expect to see targets to have at most one padding token, and ignore tokens (-100) afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7243b06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " : tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "Targets:\n",
      " : tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "batch = (inputs_1, inputs_2, inputs_3)\n",
    "print(f\"Inputs:\\n : {custom_collate_fn(batch)[0]}\")\n",
    "print(f\"Targets:\\n : {custom_collate_fn(batch)[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2127f141",
   "metadata": {},
   "source": [
    "It works as expected !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
