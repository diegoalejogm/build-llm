{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb44300e",
   "metadata": {},
   "source": [
    "### Coding Attention Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d281f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A compact self-attention class. It uses nn.Parameters to define the weight matrices Wk Wq Wv.\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class SelfAttentionV1(nn.Module):\n",
    "\n",
    "    def __init__(self, dIn, dOut):\n",
    "        super().__init__()\n",
    "        self.wKey = nn.Parameter(torch.rand(dIn, dOut))\n",
    "        self.wQuery = nn.Parameter(torch.rand(dIn, dOut))\n",
    "        self.wValue = nn.Parameter(torch.rand(dIn, dOut))\n",
    "\n",
    "    def forward(self, x):\n",
    "        K = x @ self.wKey\n",
    "        Q = x @ self.wQuery\n",
    "        V = x @ self.wValue\n",
    "\n",
    "        attnScores = Q @ K.T\n",
    "        attnWeights = torch.softmax(attnScores / K.shape[-1]**0.5, dim=-1)\n",
    "        return attnWeights @ V\n",
    "    \n",
    "    def updateWeights(self, wKey, wQuery, wValue):\n",
    "        self.wKey = nn.Parameter(wKey)\n",
    "        self.wQuery = nn.Parameter(wQuery)\n",
    "        self.wValue = nn.Parameter(wValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4bd23bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2685, 0.7413],\n",
      "        [0.2738, 0.7564],\n",
      "        [0.2668, 0.7366],\n",
      "        [0.2618, 0.7218],\n",
      "        [0.2712, 0.7495]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# Example input values to try the self-attention layer.\n",
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89],\n",
    " [0.57, 0.85, 0.64], \n",
    " [0.22, 0.58, 0.33],\n",
    " [0.77, 0.25, 0.10],\n",
    " [0.05, 0.80, 0.55]]\n",
    ")\n",
    "\n",
    "dIn, dOut = 3, 2\n",
    "print(SelfAttentionV1(dIn, dOut)(inputs))\n",
    "del inputs, dIn, dOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cdb6dbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Self-attention V2. It uses nn.Linear to define the matrices. nn.Linear is preferred as it has efficient initialization as well as optimized matrix multiplication.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mSelfAttentionV2\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dIn, dOut, qkv_bias):\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# Self-attention V2. It uses nn.Linear to define the matrices. nn.Linear is preferred as it has efficient initialization as well as optimized matrix multiplication.\n",
    "\n",
    "class SelfAttentionV2(nn.Module):\n",
    "\n",
    "    def __init__(self, dIn, dOut, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_key = nn.Linear(dIn, dOut, bias=qkv_bias)\n",
    "        self.W_query = nn.Linear(dIn, dOut, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(dIn, dOut, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute Keys, Queries, Values.\n",
    "        K = self.W_key(x)\n",
    "        Q = self.W_query(x)\n",
    "        V = self.W_value(x)\n",
    "        # Compute attention weights\n",
    "        attnScores = Q @ K.t()\n",
    "        attnWeights = torch.softmax(attnScores / K.shape[-1]**0.5, dim=-1)\n",
    "        return attnWeights @ V\n",
    "        # return attnWeights @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4417800d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m123\u001b[39m)\n\u001b[1;32m      3\u001b[0m dIn \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      4\u001b[0m dOut \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "dIn = 3\n",
    "dOut = 2\n",
    "# Example input values to try the self-attention layer.\n",
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89],\n",
    " [0.57, 0.85, 0.64], \n",
    " [0.22, 0.58, 0.33],\n",
    " [0.77, 0.25, 0.10],\n",
    " [0.05, 0.80, 0.55]]\n",
    ")\n",
    "\n",
    "print(SelfAttentionV2(dIn, dOut)(inputs))\n",
    "del inputs, dIn, dOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d8dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self Attention V2 output: \n",
      " tensor([[-0.4927, -0.0791],\n",
      "        [-0.4938, -0.0806],\n",
      "        [-0.4924, -0.0851],\n",
      "        [-0.4923, -0.0819],\n",
      "        [-0.4928, -0.0853]], grad_fn=<MmBackward0>)\n",
      "Self Attention V1 output: \n",
      " tensor([[-0.4927, -0.0791],\n",
      "        [-0.4938, -0.0806],\n",
      "        [-0.4924, -0.0851],\n",
      "        [-0.4923, -0.0819],\n",
      "        [-0.4928, -0.0853]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Validate that both attention mechanisms output the same values.\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "dIn = 3\n",
    "dOut = 2\n",
    "# Example input values to try the self-attention layer.\n",
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89],\n",
    " [0.57, 0.85, 0.64], \n",
    " [0.22, 0.58, 0.33],\n",
    " [0.77, 0.25, 0.10],\n",
    " [0.05, 0.80, 0.55]]\n",
    ")\n",
    "\n",
    "sav2 = SelfAttentionV2(dIn, dOut)\n",
    "sav1 = SelfAttentionV1(dIn, dOut)\n",
    "# Update SelfAttentionV1 to be the same as for SelfAttentionV2.\n",
    "sav1.updateWeights(sav2.W_key.weight.t(), sav2.W_query.weight.t(), sav2.W_value.weight.t())\n",
    "\n",
    "print(\"Self Attention V2 output: \\n\", sav2(inputs))\n",
    "print(\"Self Attention V1 output: \\n\", sav1(inputs))\n",
    "del inputs, dIn, dOut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f2c1c",
   "metadata": {},
   "source": [
    "#### Causal Self Attention\n",
    "\n",
    "In causal self-attention / masked attention, the model is restricted to the previous and current elements in a sequence when computing attention scores.  This is in contrast to standard self-attention which considers all elements in the sequence.\n",
    "\n",
    "It allows LLMs to learn to predict the next token in a sequence.\n",
    "\n",
    "The following implementation builds on SimpleAttentionV2, with the following changes:\n",
    "- A mask is applied to the upper diagonal elements.\n",
    "- Dropout is applied to reduce model overfitting.\n",
    "\n",
    "Drpout is typically applied at two specific times: after computing the attention weights or after applying the attention weights to the value vectors. \n",
    "\n",
    "Here we apply the attention weights after computing the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7297c387",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, dIn, dOut, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_key = nn.Linear(dIn, dOut, bias=qkv_bias)\n",
    "        self.W_query = nn.Linear(dIn, dOut, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(dIn, dOut, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # `register_buffer` helps store the mask as a non-trainable tensors\n",
    "        # within a model, never directly impacting gradients or weight updates.\n",
    "\n",
    "        self.register_buffer(\n",
    "            'mask', \n",
    "            # DiagÃ¸nal=1 ignores the diagonal from mask.\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, numTokens, dIn = x.shape \n",
    "        K = self.W_key(x) # [b, numTokens, dIn] x [dIn, dOut] = [b, numTokens, dOut]\n",
    "        Q = self.W_query(x)\n",
    "        V = self.W_value(x)\n",
    "\n",
    "        attnScores = Q @ K.transpose(1,2) # [b, numTokens, dOut] x [b, dOut, numTokens]\n",
    "        attnScores = attnScores.masked_fill(self.mask.bool()[:numTokens][:numTokens], -torch.inf)\n",
    "        attnWeights = torch.softmax(attnScores / K.shape[-1]**0.5, dim=-1)\n",
    "        # Apply dropout after computing weights, before applying them.\n",
    "        attnWeights = self.dropout(attnWeights)\n",
    "        return attnWeights @ V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "148df5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch.shape: torch.Size([2, 5, 3])\n",
      "contextVecs.shape: torch.Size([2, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "dIn = 3\n",
    "dOut = 2\n",
    "# Example input values to try the self-attention layer.\n",
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89],\n",
    " [0.57, 0.85, 0.64], \n",
    " [0.22, 0.58, 0.33],\n",
    " [0.77, 0.25, 0.10],\n",
    " [0.05, 0.80, 0.55]]\n",
    ")\n",
    "batch = torch.stack((inputs, inputs))\n",
    "\n",
    "print(\"batch.shape:\", batch.shape)\n",
    "print(\"contextVecs.shape:\", CausalAttention(dIn, dOut, batch.shape[1], 0.0)(batch).shape)\n",
    "\n",
    "del dIn, dOut, inputs, batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5728042e",
   "metadata": {},
   "source": [
    "#### Multi-head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0d6d5a",
   "metadata": {},
   "source": [
    "Multi-head attention means computing attention multiple times using different matrices. This allows the model to learn different relationships between the tokens, and attend to different parts of the sequence.\n",
    "\n",
    "As a first-approach, we implement multi-head attention in the most straightforward way. \n",
    "It is not the most efficient implementation, but allows us to try this idea.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c6fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, dIn, dOut, context_length, droput, n_heads):\n",
    "        super().__init__()\n",
    "        self.heads = [CausalAttention(dIn, dOut, context_length, droput) for i in range(n_heads)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e5f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch.shape: torch.Size([2, 5, 3])\n",
      "contextVecs.shape: torch.Size([2, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "dIn = 3\n",
    "dOut = 2\n",
    "# Example input values to try the self-attention layer.\n",
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89],\n",
    " [0.57, 0.85, 0.64], \n",
    " [0.22, 0.58, 0.33],\n",
    " [0.77, 0.25, 0.10],\n",
    " [0.05, 0.80, 0.55]]\n",
    ")\n",
    "batch = torch.stack((inputs, inputs))\n",
    "\n",
    "print(\"batch.shape:\", batch.shape)\n",
    "print(\"contextVecs.shape:\", MultiHeadAttentionWrapper(dIn, dOut, batch.shape[1], 0.0, n_heads=2)(batch).shape)\n",
    "\n",
    "del dIn, dOut, inputs, batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c7d469",
   "metadata": {},
   "source": [
    "### Efficient Multi-Head Attention\n",
    "\n",
    "Let's implement multi-head attention in a more efficent manner that does not require computing an attention matrix for each one of the heads.\n",
    "\n",
    "It consists in splitting the Keys, Queries, Values matrices into several submatrices, one per head.\n",
    "This is done by splitting the \"d_out\" dimension into \"n_heads\", \"d_head\". \n",
    "\n",
    "i.e. `(batch_size, context_size, d_out)` --> `(batch_size, context_size, n_heads, d_head)`\n",
    "\n",
    "This reshaping allows us to have a K,Q,V sub-matrix for each head using a single matrix. \n",
    "\n",
    "Next step is to reshape the matrix so we can do matrix multipilcation:\n",
    "\n",
    "i.e.  `(batch_size, context_size, n_heads, d_head)` --> `(batch_size, n_heads, context_size, d_head)`\n",
    "\n",
    "We can then use this matrix as usual to compute the attention weights. \n",
    "\n",
    "Once the outputs are computed, we must reshape the matrix to keep its original shape\n",
    "\n",
    "`(batch_size, n_heads, context_size, d_head)` --> `(batch_size, context_size, d_out)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd4eb12c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMultiHeadAttention\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dIn, dOut, numHeads, contextLength, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, qkv_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, dIn, dOut, n_heads, context_length, dropout=0.0, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (dOut % n_heads == 0), \"dOut must be divisible by n_heads\"\n",
    "\n",
    "        self.W_key = nn.Linear(dIn, dOut, bias=qkv_bias)\n",
    "        self.W_query = nn.Linear(dIn, dOut, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(dIn, dOut, bias=qkv_bias)\n",
    "        self.dHead = dOut//n_heads\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "        self.out_proj = nn.Linear(dOut, dOut)\n",
    "        self.dOut = dOut\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, numTokens, dIn = x.shape\n",
    "\n",
    "        # Compute K, Q, V vectors.\n",
    "        K = self.W_key(x) # (b, numTokens, dOut)\n",
    "        Q = self.W_query(x)\n",
    "        V = self.W_value(x)\n",
    "\n",
    "        # Unroll last matrix dimension, enabling a dimension for each head.\n",
    "        K = K.view(b, numTokens, self.n_heads, self.dHead) # (b, numTokens, n_heads, dHead)\n",
    "        Q = Q.view(b, numTokens, self.n_heads, self.dHead)\n",
    "        V = V.view(b, numTokens, self.n_heads, self.dHead)\n",
    "\n",
    "        # Transpose matrices, making n_heads the second dim.\n",
    "        K = K.transpose(1, 2) # (b, n_heads, numTokens, dHead)\n",
    "        Q = Q.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores.\n",
    "        attnScores = Q @ K.transpose(2,3) # = (b, n_heads, numTokens, numTokens)\n",
    "        maskBool = self.mask.bool()[:numTokens,:numTokens]\n",
    "        # Apply mask\n",
    "        attnScores.masked_fill_(maskBool, -torch.inf)\n",
    "\n",
    "        # Compute attention weights.\n",
    "        attnWeights = torch.softmax(attnScores / K.shape[-1]**0.5, dim=-1)\n",
    "        attnWeights = self.dropout(attnWeights)\n",
    "\n",
    "        # Compute context vector.\n",
    "        context = attnWeights @ V # (b, n_heads, numTokens, dHead)\n",
    "        context = context.transpose(1,2) # (b, numTokens, n_heads, dHead)\n",
    "        context = context.contiguous().view(b, numTokens, self.dOut) # (b, numTokens, dOut)\n",
    "        return self.out_proj(context) # (b, numTokens, dOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9ca5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch.shape: torch.Size([2, 5, 3])\n",
      "contextVecs.shape: torch.Size([2, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "dIn = 3\n",
    "dOut = 2\n",
    "# Example input values to try the self-attention layer.\n",
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89],\n",
    " [0.57, 0.85, 0.64], \n",
    " [0.22, 0.58, 0.33],\n",
    " [0.77, 0.25, 0.10],\n",
    " [0.05, 0.80, 0.55]]\n",
    ")\n",
    "batch = torch.stack((inputs, inputs))\n",
    "\n",
    "print(\"batch.shape:\", batch.shape)\n",
    "print(\"contextVecs.shape:\", MultiHeadAttention(dIn, dOut, context_length=5, n_heads=2)(batch).shape)\n",
    "\n",
    "del dIn, dOut, inputs, batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2401e2f2",
   "metadata": {},
   "source": [
    "### Exercise 3.3 Initializing GPT-2 size attention modules\n",
    "\n",
    "Initialize a multi-head attention module that has the same numer of attention heads as the smallest GPT-2 model (12 attention heads). Also ensurr that you use the respective input and output embedding sisze simlar to GPT-2 (768 dimensions). Note that the smallest GPT-2 model supports a context length of 1,024 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f0c9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch.shape: torch.Size([2, 1024, 768])\n",
      "contextVecs.shape: torch.Size([2, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "dIn = 768\n",
    "dOut = 768\n",
    "# Example input values to try the self-attention layer.\n",
    "inputs =torch.ones((1024,768))\n",
    "batch = torch.stack((inputs, inputs))\n",
    "\n",
    "print(\"batch.shape:\", batch.shape)\n",
    "print(\"contextVecs.shape:\", MultiHeadAttention(dIn, dOut, context_length=1024, n_heads=12)(batch).shape)\n",
    "\n",
    "del dIn, dOut, inputs, batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
