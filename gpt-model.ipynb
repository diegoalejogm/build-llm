{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0398ef5",
   "metadata": {},
   "source": [
    "# GPT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8000ece1",
   "metadata": {},
   "source": [
    "In this notebook we will implement the GPT-2 model. \n",
    "\n",
    "To start, we implement at dummy class that shows the architecture of the neural network. \n",
    "\n",
    "Then, we implement the layers we mocked in the dummy GPT model.\n",
    "\n",
    "Finally, we put everything back together with a few additions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85427293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637592a8",
   "metadata": {},
   "source": [
    "## Dummy GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3089cf",
   "metadata": {},
   "source": [
    "We implement a dummy GPT class below, with the following architecture:\n",
    "\n",
    "\n",
    "Architecture:\n",
    "```\n",
    "DummyGPT(\n",
    "  (dropout): Dropout(p=0.1, inplace=False)\n",
    "  (tokenEmbed): Embedding(50257, 768)\n",
    "  (posEmbed): Embedding(1024, 768)\n",
    "  (normLayer): DummyLayerNorm()\n",
    "  (trfBlocks): Sequential(\n",
    "    (0): DummyTransformer()\n",
    "    (1): DummyTransformer()\n",
    "    (2): DummyTransformer()\n",
    "    (3): DummyTransformer()\n",
    "    (4): DummyTransformer()\n",
    "    (5): DummyTransformer()\n",
    "    (6): DummyTransformer()\n",
    "    (7): DummyTransformer()\n",
    "    (8): DummyTransformer()\n",
    "    (9): DummyTransformer()\n",
    "    (10): DummyTransformer()\n",
    "    (11): DummyTransformer()\n",
    "  )\n",
    "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
    ")\n",
    "```\n",
    "\n",
    "The model will be initialized using a configuration dictionary that will contain the number of layers and other model parameters. \n",
    "\n",
    "We use the parameters from GPT-2 for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2302c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"vocab_size\": 50257, # Number of tokens in vocabulary.\n",
    "    \"context_length\": 256, # Max. number of tokens the LLM sees per run.\n",
    "    \"emb_dim\": 768, # Size of the internal. embeddings used in the LLM attention mechanism.\n",
    "    \"n_layers\" : 12, # Number of transformer layers.\n",
    "    \"drop_rate\": 0.1, # Feature dropout rate.\n",
    "    \"n_heads\": 12, # Num. Attention heads per multi-head attention block\n",
    "    \"qkv_bias\" : False,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44577105",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class DummyTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce4c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPT(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        # Token embedding\n",
    "        self.tokenEmbed = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        # Positional embedding\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        # Normalization\n",
    "        self.normLayer = DummyLayerNorm()\n",
    "        # Transformer Blocks\n",
    "        self.trf_blocks = nn.Sequential(*[\n",
    "            DummyTransformer() for _ in range(cfg[\"n_layers\"])\n",
    "        ])\n",
    "        # Linear projection.\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, seqLen = x.shape\n",
    "        tokEmbeds = self.tokenEmbed(x)\n",
    "        posEmbeds = self.pos_emb(torch.arange(seqLen, device=x.device))\n",
    "        x = tokEmbeds + posEmbeds\n",
    "        x = self.dropout(x)\n",
    "        x = self.normLayer(x)\n",
    "        for l in self.trf_blocks:\n",
    "            x = l(x)\n",
    "        return self.out_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6e7bb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyGPT(\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (tokenEmbed): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (normLayer): DummyLayerNorm()\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): DummyTransformer()\n",
       "    (1): DummyTransformer()\n",
       "    (2): DummyTransformer()\n",
       "    (3): DummyTransformer()\n",
       "    (4): DummyTransformer()\n",
       "    (5): DummyTransformer()\n",
       "    (6): DummyTransformer()\n",
       "    (7): DummyTransformer()\n",
       "    (8): DummyTransformer()\n",
       "    (9): DummyTransformer()\n",
       "    (10): DummyTransformer()\n",
       "    (11): DummyTransformer()\n",
       "  )\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model.\n",
    "dummyGpt = DummyGPT(cfg)\n",
    "dummyGpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b820bd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a batch to be used for testing purposes\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a67fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "private privileDust proclaimed\n",
      "IsnAPH guided predecessors\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and run model on batch.\n",
    "# Model isn't trained and all ouputs are gibberish.\n",
    "torch.manual_seed(123)\n",
    "\n",
    "logits = dummyGpt(batch)  # shape: [batch, seqLen, vocab_size]\n",
    "pred_ids = logits.argmax(dim=-1)  # shape: [batch, seqLen]\n",
    "print(batch)\n",
    "for i in range(pred_ids.shape[0]):\n",
    "    print(tokenizer.decode(pred_ids[i].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f6b6a2",
   "metadata": {},
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53648fa0",
   "metadata": {},
   "source": [
    "Next step is to implement the layer normalization class.\n",
    "\n",
    "Layer normalization is used to keep a stable model training, by reducing internal co-variate shift.  \n",
    "\n",
    "Let's explain in more details:\n",
    "\n",
    "- Covariate: Refers to the input features of a ML model.\n",
    "- Shift : Distribution change.\n",
    "- Internal : Refers to the internal covariates, which are the internal activations of the neural networks.\n",
    "\n",
    "\n",
    "### Why does layer normalization help reduce internal covariate shift ?\n",
    "\n",
    "While the model is trained, model weights are updated at each training step. Without normalization, each layer is forced to learn the new distribution of the previous layer outputs. Layer normalization helps by making the mean = 0 and variance = 1, maintaining the input feature distribution regardless of the iteration.\n",
    "\n",
    "### How does it work ?\n",
    "\n",
    "For an input vector `X = [x1, x2, ..., xn]`, layer normalization works by computing:\n",
    "\n",
    "`z =  (  xi  - mean(X)  )  /  std(X) `\n",
    "\n",
    "In practice, layer normalization includes two learnable terms that allow the model to rescale and shift the inputs, learning better representations of the input activations:\n",
    "\n",
    "`z = γ * (  xi  - mean(X)  )  /  ( std(X) + ε )    +    β `\n",
    "\n",
    "Where:\n",
    "- `γ`: Scale\n",
    "- `ε`: Small Epsilon, avoid division by 0.\n",
    "- `β`: Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f94f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        # Scale parameter.\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        # Shift parameter.\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        # Epsilon constant. Avoids division by zero.\n",
    "        self.eps = 1e-8\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var  = x.var(dim=-1, keepdim=True, unbiased=False) \n",
    "        normX = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * normX + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8db948e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      "  tensor([-0.3596, -0.2606])\n",
      "Variance :\n",
      "  tensor([0.2015, 0.2673])\n",
      "Norm. Mean:\n",
      "  tensor([    -0.0000,      0.0000], grad_fn=<MeanBackward1>)\n",
      "Norm. Variance :\n",
      "  tensor([1.0000, 1.0000], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Try LayerNorm.\n",
    "torch.manual_seed(123)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "batch = torch.randn(2, 5)\n",
    "print(\"Mean:\\n \", batch.mean(dim=-1))\n",
    "print(\"Variance :\\n \", batch.var(dim=-1,  unbiased=False))\n",
    "\n",
    "ln = LayerNorm(5)\n",
    "out = ln(batch)\n",
    "print(\"Norm. Mean:\\n \", out.mean(dim=-1))\n",
    "print(\"Norm. Variance :\\n \", out.var(dim=-1,  unbiased=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f4703",
   "metadata": {},
   "source": [
    "## GELU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc291da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2/torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x,3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfc19919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0508,  0.0659, -0.1315, -0.0974, -0.1387],\n",
       "        [ 0.1220, -0.1610, -0.1700,  0.2031, -0.0496]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GELU()(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9317f05",
   "metadata": {},
   "source": [
    "## FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e16110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardModule(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        emb_dim = cfg[\"emb_dim\"]\n",
    "\n",
    "        # Sequence of layers, linear -> GELU -> linear. \n",
    "        # Note that the first linear layer expands the dimension of the input,\n",
    "        # whereas the second one, reduces the dimension to the original embed-dimension size.\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4 * emb_dim),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * emb_dim, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec1d87c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0692,  0.1527, -0.0808,  0.0560,  0.1473],\n",
       "        [ 0.0020,  0.1303, -0.2325,  0.0055,  0.2685]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FeedForwardModule({\"emb_dim\": 5})(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3994120a",
   "metadata": {},
   "source": [
    "## Transformer Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2422d3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2685, 0.7413],\n",
      "        [0.2738, 0.7564],\n",
      "        [0.2668, 0.7366],\n",
      "        [0.2618, 0.7218],\n",
      "        [0.2712, 0.7495]], grad_fn=<MmBackward0>)\n",
      "tensor([[-0.4927, -0.0791],\n",
      "        [-0.4938, -0.0806],\n",
      "        [-0.4924, -0.0851],\n",
      "        [-0.4923, -0.0819],\n",
      "        [-0.4928, -0.0853]], grad_fn=<MmBackward0>)\n",
      "Self Attention V2 output: \n",
      " tensor([[-0.4927, -0.0791],\n",
      "        [-0.4938, -0.0806],\n",
      "        [-0.4924, -0.0851],\n",
      "        [-0.4923, -0.0819],\n",
      "        [-0.4928, -0.0853]], grad_fn=<MmBackward0>)\n",
      "Self Attention V1 output: \n",
      " tensor([[-0.4927, -0.0791],\n",
      "        [-0.4938, -0.0806],\n",
      "        [-0.4924, -0.0851],\n",
      "        [-0.4923, -0.0819],\n",
      "        [-0.4928, -0.0853]], grad_fn=<MmBackward0>)\n",
      "batch.shape: torch.Size([2, 5, 3])\n",
      "contextVecs.shape: torch.Size([2, 5, 2])\n",
      "batch.shape: torch.Size([2, 5, 3])\n",
      "contextVecs.shape: torch.Size([2, 5, 4])\n",
      "batch.shape: torch.Size([2, 5, 3])\n",
      "contextVecs.shape: torch.Size([2, 5, 2])\n",
      "batch.shape: torch.Size([2, 1024, 768])\n",
      "contextVecs.shape: torch.Size([2, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "# Load attention modules from previous ipynb.\n",
    "%run coding-attention-mechanisms.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3656347e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d209a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        emb_dim = cfg[\"emb_dim\"]\n",
    "        self.norm1 = LayerNorm(emb_dim)\n",
    "        self.norm2 = LayerNorm(emb_dim)\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.ff = FeedForwardModule(cfg)\n",
    "        self.att = MultiHeadAttention(\n",
    "            emb_dim, emb_dim, \n",
    "            context_length=cfg[\"context_length\"],\n",
    "            n_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        # LayerNorm\n",
    "        x = self.norm1(x)\n",
    "        # MultiHeadAttention\n",
    "        x = self.att(x)\n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "        # Shortcut\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        # LayerNorm\n",
    "        x = self.norm2(x)\n",
    "        # Forward\n",
    "        x = self.ff(x)\n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "        # Shortcut\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "347a9cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9744, 0.7451, 1.1016, 1.0350, 1.0850, 1.0020, 0.8070, 0.7283,\n",
       "          1.0585, 1.0846]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test transformer block\n",
    "TransformerBlock(cfg)(torch.ones(1,1,768))[:,:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8e2b54",
   "metadata": {},
   "source": [
    "# GPT Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0198417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.trf_blocks = nn.Sequential(*[\n",
    "            TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TokEmbed & PosEmbed\n",
    "        b, seqLen = x.shape\n",
    "        tokens = self.tok_emb(x)\n",
    "        pos = self.pos_emb(torch.arange(seqLen, device=x.device))\n",
    "        x = tokens + pos\n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "        # Transformer[]\n",
    "        x = self.trf_blocks(x)\n",
    "        # LayerNorm\n",
    "        x = self.final_norm(x)\n",
    "        # Output heads\n",
    "        logits = self.out_head(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43065ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the GPT Moduel\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "\n",
    "model = GPTModel(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f6003c",
   "metadata": {},
   "source": [
    "### Generating Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1eaebd",
   "metadata": {},
   "source": [
    "- **ArgMax**: Choose the token with the highest score.\n",
    "\n",
    "- **TopK**: Choose only from the topK token scores.\n",
    "\n",
    "- **Temperature**: Logits are divided by temperature. A temperature > 1 scales values, making them more likely. A temperature < 1 makes tokens with highest score more likely to be selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0115837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that runs the model `max_new_tokens` times.\n",
    "# It appends the predicted text to the input `idx` text. \n",
    "# The output is encoded, it needs to be decoded with the tokenizer.\n",
    "def generateText(model: nn.Module, idx: tuple, max_new_tokens: int, context_size: int, \n",
    "                 temperature : int = 0.0, top_k=None, eos_id=None):\n",
    "    for i in range(max_new_tokens):\n",
    "        idxCond = idx[:, -context_size:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(idxCond)\n",
    "        logits = logits[:, -1, :]\n",
    "               \n",
    "        # Apply topk if specified.\n",
    "        if top_k:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, \n",
    "                                 torch.tensor(float('-inf')).to(logits.device),\n",
    "                                 logits)\n",
    "\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probas = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probas, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49f6261b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# Generate start tensor.\n",
    "startContext = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(startContext)\n",
    "print(\"encoded:\", encoded)\n",
    "encodedTensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encodedTensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "790d1582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 24111, 43446, 12663, 18650, 28505, 27960]])\n",
      "Output length: 10\n",
      "Hello, I amFrench rebirth fundra Oracle Worm Midnight\n"
     ]
    }
   ],
   "source": [
    "# Run GPT and decode outputs.\n",
    "model.eval()\n",
    "out = generateText(\n",
    "    model=model,\n",
    "    idx=encodedTensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=cfg[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))\n",
    "decodedText = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decodedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f90e03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
