{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0398ef5",
   "metadata": {},
   "source": [
    "# GPT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8000ece1",
   "metadata": {},
   "source": [
    "In this notebook we will implement the GPT-2 model. \n",
    "\n",
    "To start, we implement at dummy class that shows the architecture of the neural network. \n",
    "\n",
    "Then, we implement the layers we mocked in the dummy GPT model.\n",
    "\n",
    "Finally, we put everything back together with a few additions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "85427293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637592a8",
   "metadata": {},
   "source": [
    "## Dummy GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3089cf",
   "metadata": {},
   "source": [
    "We implement a dummy GPT class below, with the following architecture:\n",
    "\n",
    "\n",
    "Architecture:\n",
    "```\n",
    "DummyGPT(\n",
    "  (dropout): Dropout(p=0.1, inplace=False)\n",
    "  (tokenEmbed): Embedding(50257, 768)\n",
    "  (posEmbed): Embedding(1024, 768)\n",
    "  (normLayer): DummyLayerNorm()\n",
    "  (trfBlocks): Sequential(\n",
    "    (0): DummyTransformer()\n",
    "    (1): DummyTransformer()\n",
    "    (2): DummyTransformer()\n",
    "    (3): DummyTransformer()\n",
    "    (4): DummyTransformer()\n",
    "    (5): DummyTransformer()\n",
    "    (6): DummyTransformer()\n",
    "    (7): DummyTransformer()\n",
    "    (8): DummyTransformer()\n",
    "    (9): DummyTransformer()\n",
    "    (10): DummyTransformer()\n",
    "    (11): DummyTransformer()\n",
    "  )\n",
    "  (outLayer): Linear(in_features=768, out_features=50257, bias=False)\n",
    ")\n",
    "```\n",
    "\n",
    "The model will be initialized using a configuration dictionary that will contain the number of layers and other model parameters. \n",
    "\n",
    "We use the parameters from GPT-2 for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7e2302c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"vocabSize\": 50257, # Number of tokens in vocabulary.\n",
    "    \"contextLength\": 1024, # Max. number of tokens the LLM sees per run.\n",
    "    \"embedDim\": 768, # Size of the internal. embeddings used in the LLM attention mechanism.\n",
    "    \"nLayers\" : 12, # Number of transformer layers.\n",
    "    \"dropRate\": 0.1, # Feature dropout rate.\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "44577105",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class DummyTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "dce4c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPT(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(cfg[\"dropRate\"])\n",
    "        # Token embedding\n",
    "        self.tokenEmbed = nn.Embedding(cfg[\"vocabSize\"], cfg[\"embedDim\"])\n",
    "        # Positional embedding\n",
    "        self.posEmbed = nn.Embedding(cfg[\"contextLength\"], cfg[\"embedDim\"])\n",
    "        # Normalization\n",
    "        self.normLayer = DummyLayerNorm()\n",
    "        # Transformer Blocks\n",
    "        self.trfBlocks = nn.Sequential(*[\n",
    "            DummyTransformer() for _ in range(cfg[\"nLayers\"])\n",
    "        ])\n",
    "        # Linear projection.\n",
    "        self.outLayer = nn.Linear(cfg[\"embedDim\"], cfg[\"vocabSize\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, seqLen = x.shape\n",
    "        tokEmbeds = self.tokenEmbed(x)\n",
    "        posEmbeds = self.posEmbed(torch.arange(seqLen, device=x.device))\n",
    "        x = tokEmbeds + posEmbeds\n",
    "        x = self.dropout(x)\n",
    "        x = self.normLayer(x)\n",
    "        for l in self.trfBlocks:\n",
    "            x = l(x)\n",
    "        return self.outLayer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b6e7bb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyGPT(\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (tokenEmbed): Embedding(50257, 768)\n",
       "  (posEmbed): Embedding(1024, 768)\n",
       "  (normLayer): DummyLayerNorm()\n",
       "  (trfBlocks): Sequential(\n",
       "    (0): DummyTransformer()\n",
       "    (1): DummyTransformer()\n",
       "    (2): DummyTransformer()\n",
       "    (3): DummyTransformer()\n",
       "    (4): DummyTransformer()\n",
       "    (5): DummyTransformer()\n",
       "    (6): DummyTransformer()\n",
       "    (7): DummyTransformer()\n",
       "    (8): DummyTransformer()\n",
       "    (9): DummyTransformer()\n",
       "    (10): DummyTransformer()\n",
       "    (11): DummyTransformer()\n",
       "  )\n",
       "  (outLayer): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model.\n",
    "dummyGpt = DummyGPT(cfg)\n",
    "dummyGpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b820bd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a batch to be used for testing purposes\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f0a67fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      " bargainLogin UNIVERS Trident\n",
      " bargain Olsenconfidencemes\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and run model on batch.\n",
    "# Model isn't trained and all ouputs are gibberish.\n",
    "torch.manual_seed(123)\n",
    "\n",
    "logits = dummyGpt(batch)  # shape: [batch, seqLen, vocabSize]\n",
    "pred_ids = logits.argmax(dim=-1)  # shape: [batch, seqLen]\n",
    "print(batch)\n",
    "for i in range(pred_ids.shape[0]):\n",
    "    print(tokenizer.decode(pred_ids[i].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f6b6a2",
   "metadata": {},
   "source": [
    "# Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53648fa0",
   "metadata": {},
   "source": [
    "Next step is to implement the layer normalization class.\n",
    "\n",
    "Layer normalization is used to keep a stable model training, by reducing internal co-variate shift.  \n",
    "\n",
    "Let's explain in more details:\n",
    "\n",
    "- Covariate: Refers to the input features of a ML model.\n",
    "- Shift : Distribution change.\n",
    "- Internal : Refers to the internal covariates, which are the internal activations of the neural networks.\n",
    "\n",
    "\n",
    "### Why does layer normalization help reduce internal covariate shift ?\n",
    "\n",
    "While the model is trained, model weights are updated at each training step. Without normalization, each layer is forced to learn the new distribution of the previous layer outputs. Layer normalization helps by making the mean = 0 and variance = 1, maintaining the input feature distribution regardless of the iteration.\n",
    "\n",
    "### How does it work ?\n",
    "\n",
    "For an input vector `X = [x1, x2, ..., xn]`, layer normalization works by computing:\n",
    "\n",
    "`z =  (  xi  - mean(X)  )  /  std(X) `\n",
    "\n",
    "In practice, layer normalization includes two learnable terms that allow the model to rescale and shift the inputs, learning better representations of the input activations:\n",
    "\n",
    "`z = γ * (  xi  - mean(X)  )  /  ( std(X) + ε )    +    β `\n",
    "\n",
    "Where:\n",
    "- `γ`: Scale\n",
    "- `ε`: Small Epsilon, avoid division by 0.\n",
    "- `β`: Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f3f94f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, embDim):\n",
    "        super().__init__()\n",
    "        # Scale parameter.\n",
    "        self.scale = nn.Parameter(torch.ones(embDim))\n",
    "        # Shift parameter.\n",
    "        self.shift = nn.Parameter(torch.zeros(embDim))\n",
    "        # Epsilon constant. Avoids division by zero.\n",
    "        self.eps = 1e-8\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var  = x.var(dim=-1, keepdim=True, unbiased=False) \n",
    "        normX = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * normX + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "f8db948e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      "  tensor([ 0.2662, -0.7222])\n",
      "Variance :\n",
      "  tensor([0.5920, 1.1845])\n",
      "Norm. Mean:\n",
      "  tensor([0., 0.], grad_fn=<MeanBackward1>)\n",
      "Norm. Variance :\n",
      "  tensor([1.0000, 1.0000], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Try LayerNorm.\n",
    "torch.manual_seed(123)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "batch = torch.randn(2, 10)\n",
    "print(\"Mean:\\n \", batch.mean(dim=-1))\n",
    "print(\"Variance :\\n \", batch.var(dim=-1,  unbiased=False))\n",
    "\n",
    "ln = LayerNorm(10)\n",
    "out = ln(batch)\n",
    "print(\"Norm. Mean:\\n \", out.mean(dim=-1))\n",
    "print(\"Norm. Variance :\\n \", out.var(dim=-1,  unbiased=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
