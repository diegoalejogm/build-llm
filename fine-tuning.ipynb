{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1b33deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1816617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection/SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "def download_and_unzip_spam_data(\n",
    "    url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download \"\n",
    "            \"and extraction.\"\n",
    "        )\n",
    "        return\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11bb8f7",
   "metadata": {},
   "source": [
    "#### Load into a Pandas DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea640ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                               Text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e50e88e",
   "metadata": {},
   "source": [
    "#### Examine value distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da49fc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fd3205",
   "metadata": {},
   "source": [
    "There is a class inbalance. For ease, we will keep a 50:50 class-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e72f167b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "spam    747\n",
      "ham     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def create_balanced_dataset(df):\n",
    "    spam_df = df[df[\"Label\"] == \"spam\"]\n",
    "    ham_df = df[df[\"Label\"] == \"ham\"].sample(n=len(spam_df), random_state=123)\n",
    "    balanced_df = pd.concat([spam_df, ham_df])\n",
    "    return balanced_df\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2144024",
   "metadata": {},
   "source": [
    "Map labels into integers {1, 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff40a7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>SIX chances to win CASH! From 100 to 20,000 po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4707</th>\n",
       "      <td>0</td>\n",
       "      <td>Wow so healthy. Old airport rd lor. Cant thk o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3293</th>\n",
       "      <td>0</td>\n",
       "      <td>Dear good morning how you feeling dear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>0</td>\n",
       "      <td>Dont put your phone on silent mode ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4079</th>\n",
       "      <td>0</td>\n",
       "      <td>Gam gone after outstanding innings.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4468</th>\n",
       "      <td>0</td>\n",
       "      <td>She said,'' do u mind if I go into the bedroom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1494 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label                                               Text\n",
       "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "5         1  FreeMsg Hey there darling it's been 3 week's n...\n",
       "8         1  WINNER!! As a valued network customer you have...\n",
       "9         1  Had your mobile 11 months or more? U R entitle...\n",
       "11        1  SIX chances to win CASH! From 100 to 20,000 po...\n",
       "...     ...                                                ...\n",
       "4707      0  Wow so healthy. Old airport rd lor. Cant thk o...\n",
       "3293      0             Dear good morning how you feeling dear\n",
       "1278      0              Dont put your phone on silent mode ok\n",
       "4079      0                Gam gone after outstanding innings.\n",
       "4468      0  She said,'' do u mind if I go into the bedroom...\n",
       "\n",
       "[1494 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "balanced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013a3509",
   "metadata": {},
   "source": [
    "##### Create random split for training, validation, test (70%, 10%, 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db91d2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1045\n",
      "Validation size: 150\n",
      "Test size: 299\n"
     ]
    }
   ],
   "source": [
    "def random_split(df, train_frac=0.7, val_frac=0.1, random_state=123):\n",
    "    # Shuffle the DataFrame.\n",
    "    df = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    val_end = int(len(df) * (train_frac + val_frac))\n",
    "    df_train = df[:train_end]\n",
    "    df_val = df[train_end:val_end]\n",
    "    df_test = df[val_end:]\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "df_train, df_val, df_test = random_split(balanced_df)\n",
    "print(f\"Train size: {len(df_train)}\")\n",
    "print(f\"Validation size: {len(df_val)}\")\n",
    "print(f\"Test size: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57eecbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV files.\n",
    "df_train.to_csv(\"train.csv\", index=None)\n",
    "df_val.to_csv(\"validation.csv\", index=None)\n",
    "df_test.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2039d0e4",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b8055a",
   "metadata": {},
   "source": [
    "We now need to create a dataloader to ingest the data into the LLM. \n",
    "\n",
    "Note that the text messages may have different sizes, and thus we will pad all messages to have the same length in a given batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2bb668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_token_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "\n",
    "        # Encode all texts in the dataset.\n",
    "        self.encoded_texts = [tokenizer.encode(text) for text in self.data[\"Text\"]]\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            # Truncate texts that are longer than max_length.\n",
    "            self.encoded_texts = [enc[:max_length] for enc in self.encoded_texts]\n",
    "        \n",
    "        # Pad all encoded texts to max_length.\n",
    "        self.encoded_texts = [\n",
    "            enc + [self.pad_token_id] * (self.max_length - len(enc)) for enc in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = torch.tensor(self.encoded_texts[idx], dtype=torch.long)\n",
    "        label = torch.tensor(self.data.iloc[idx][\"Label\"], dtype=torch.long)\n",
    "        return text, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        return max(len(enc) for enc in self.encoded_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bb875b",
   "metadata": {},
   "source": [
    "##### Load Train, Validation and Test sets into the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00dc50b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Number of tokens in the longest sequence: 120\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "train_dataset = SpamDataset(\"train.csv\", max_length=None, tokenizer=tokenizer)\n",
    "print( \"Train: Number of tokens in the longest sequence:\", train_dataset.max_length)\n",
    "\n",
    "val_dataset = SpamDataset(\"validation.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
    "test_dataset = SpamDataset(\"test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac2c878",
   "metadata": {},
   "source": [
    "##### TODO: Exercise 6.1 Increasing the context length\n",
    "Pad the inputs to the maximum number of tokens the model supports and observe\n",
    "how it affects the predictive performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbbedd6",
   "metadata": {},
   "source": [
    "### Creating Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45c49783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 8\n",
    "num_workers = 0\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=num_workers, \n",
    "    shuffle=True, \n",
    "    drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=num_workers, \n",
    "    shuffle=False, \n",
    "    drop_last=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=num_workers, \n",
    "    shuffle=False, \n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8628f09",
   "metadata": {},
   "source": [
    "To get an idea of the batch shapes, let's print them down below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c56e3f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "for input_batch, target_batch in train_loader:\n",
    "    break\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions:\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad8c8c9",
   "metadata": {},
   "source": [
    "To get an idea of the dataset size, let's print it down below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61aac7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 batches in the training DataLoader\n",
      "19 batches in the validation DataLoader\n",
      "38 batches in the test DataLoader\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader), \"batches in the training DataLoader\")\n",
    "print(len(val_loader), \"batches in the validation DataLoader\")\n",
    "print(len(test_loader), \"batches in the test DataLoader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f8b80a",
   "metadata": {},
   "source": [
    "#### Load GPT2 Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42e981b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12, \"file_name\": \"gpt2-small-124M.pth\"},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16, \"file_name\": \"gpt2-medium-355M.pth\"},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20, \"file_name\": \"gpt2-large-774M.pth\"},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25, \"file_name\": \"gpt2-xl-1558M.pth\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad7324a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba32020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-xl (1558M)\"\n",
    "file_name = model_configs[model_name][\"file_name\"]\n",
    "BASE_CONFIG.update(model_configs[model_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b360ab4b",
   "metadata": {},
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d2c95d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      " Jiang exclude intensimet\n",
      " B LeavingACC Deng\n",
      "Mean:\n",
      "  tensor([-0.3596, -0.2606])\n",
      "Variance :\n",
      "  tensor([0.2015, 0.2673])\n",
      "Norm. Mean:\n",
      "  tensor([    -0.0000,      0.0000], grad_fn=<MeanBackward1>)\n",
      "Norm. Variance :\n",
      "  tensor([1.0000, 1.0000], grad_fn=<VarBackward0>)\n",
      "tensor([[0.2685, 0.7413],\n",
      "        [0.2738, 0.7564],\n",
      "        [0.2668, 0.7366],\n",
      "        [0.2618, 0.7218],\n",
      "        [0.2712, 0.7495]], grad_fn=<MmBackward0>)\n",
      "tensor([[-0.4927, -0.0791],\n",
      "        [-0.4938, -0.0806],\n",
      "        [-0.4924, -0.0851],\n",
      "        [-0.4923, -0.0819],\n",
      "        [-0.4928, -0.0853]], grad_fn=<MmBackward0>)\n",
      "Self Attention V2 output: \n",
      " tensor([[-0.4927, -0.0791],\n",
      "        [-0.4938, -0.0806],\n",
      "        [-0.4924, -0.0851],\n",
      "        [-0.4923, -0.0819],\n",
      "        [-0.4928, -0.0853]], grad_fn=<MmBackward0>)\n",
      "Self Attention V1 output: \n",
      " tensor([[-0.4927, -0.0791],\n",
      "        [-0.4938, -0.0806],\n",
      "        [-0.4924, -0.0851],\n",
      "        [-0.4923, -0.0819],\n",
      "        [-0.4928, -0.0853]], grad_fn=<MmBackward0>)\n",
      "batch.shape: torch.Size([2, 5, 3])\n",
      "contextVecs.shape: torch.Size([2, 5, 2])\n",
      "batch.shape: torch.Size([2, 5, 3])\n",
      "contextVecs.shape: torch.Size([2, 5, 4])\n",
      "batch.shape: torch.Size([2, 5, 3])\n",
      "contextVecs.shape: torch.Size([2, 5, 2])\n",
      "batch.shape: torch.Size([2, 1024, 768])\n",
      "contextVecs.shape: torch.Size([2, 1024, 768])\n",
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n",
      "Output: tensor([[15496,    11,   314,   716, 24111, 43446, 11311, 42014, 15660, 43819]])\n",
      "Output length: 10\n",
      "Hello, I amFrench rebirth chocolate horny incentiveliterally\n",
      "year 1122\n",
      "years 1123\n",
      "yellow 1124\n",
      "yet 1125\n",
      "you 1126\n",
      "younger 1127\n",
      "your 1128\n",
      "yourself 1129\n",
      "<|endoftext|> 1130\n",
      "<|unk|> 1131\n",
      "[999, 1131, 1131, 1130, 584, 0, 0, 1077, 6]\n",
      "this <|unk|> <|unk|> <|endoftext|> is!! was--\n",
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  198, 11274,  5891,  1576],\n",
      "        [  438,   568,   340,   373],\n",
      "        [  645,  1049,  5975,   284],\n",
      "        [  502,   284,  3285,   326]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   198],\n",
      "        [11274,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "torch.Size([8, 4, 256])\n",
      "torch.Size([4, 256])\n",
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# Load GPT model\n",
    "%run gpt-model.ipynb\n",
    "%run data-processing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fd32fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    return torch.tensor(encoded).unsqueeze(0)\n",
    "\n",
    "def token_ids_to_text(tokens, tokenizer):\n",
    "    formatted = tokens.squeeze(0).tolist()\n",
    "    return tokenizer.decode(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d83055a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1600)\n",
       "  (pos_emb): Embedding(1024, 1600)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (24): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (25): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (26): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (27): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (28): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (29): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (30): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (31): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (32): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (33): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (34): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (35): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (36): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (37): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (38): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (39): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (40): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (41): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (42): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (43): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (44): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (45): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (46): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (47): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff): FeedForwardModule(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = f\"https://huggingface.co/rasbt/gpt2-from-scratch-pytorch/resolve/main/{file_name}\"\n",
    "\n",
    "if not os.path.exists(file_name):\n",
    "    urllib.request.urlretrieve(url, file_name)\n",
    "    print(f\"Downloaded to {file_name}\")\n",
    "\n",
    "gpt = GPTModel(BASE_CONFIG)\n",
    "gpt.load_state_dict(torch.load(file_name, weights_only=True))\n",
    "gpt.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a0009e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you closer… You are all here for us… Please take us to God!\" she cried.\n",
      "But they knew to hold the\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generateText(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6243e31",
   "metadata": {},
   "source": [
    "Let's test whether the model is good at classifying spam already, before doing any fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11958b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.' clans Chester seasonal borrowed elevate tou Sorazek Hats Arkham subreddit manipulated Belle famously appreciation declaresildo lich PowerShell expireucked SHnesday\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "token_ids = generateText(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens=23,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2624b114",
   "metadata": {},
   "source": [
    "As you can see, the model is not able to classify the text into spam or not spam, it just autocompletes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ed9a51",
   "metadata": {},
   "source": [
    "### Adding a Classification Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cca81a",
   "metadata": {},
   "source": [
    "Let's first display the structure of the model.\n",
    "As expected, the last layer of the model has an output size of ~50k, equivalent to the size of the model vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c73114dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 1600)\n",
      "  (pos_emb): Embedding(1024, 1600)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (12): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (13): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (14): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (15): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (16): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (17): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (18): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (19): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (20): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (21): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (22): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (23): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (24): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (25): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (26): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (27): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (28): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (29): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (30): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (31): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (32): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (33): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (34): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (35): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (36): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (37): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (38): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (39): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (40): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (41): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (42): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (43): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (44): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (45): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (46): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (47): TransformerBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (ff): FeedForwardModule(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_key): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_query): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (W_value): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd02844",
   "metadata": {},
   "source": [
    "##### Freeze all weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fb315bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc596dd",
   "metadata": {},
   "source": [
    "##### Replace classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2752cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 2\n",
    "# This new layer has requires_grad=True by default.\n",
    "gpt.out_head = torch.nn.Linear(BASE_CONFIG[\"emb_dim\"], num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bab653",
   "metadata": {},
   "source": [
    "##### Unfreeze last transformer block and final layer norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb07c9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in gpt.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in gpt.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64efb120",
   "metadata": {},
   "source": [
    "##### Test the new model output shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc431ce",
   "metadata": {},
   "source": [
    "Let's first create the input dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8d5f5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[5211,  345,  423,  640]])\n",
      "Inputs dimensions: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Inputs dimensions:\", inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702cae4f",
   "metadata": {},
   "source": [
    "The last output dimension is now `2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56f5332a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:\n",
      " tensor([[[ 0.1749,  0.6254],\n",
      "         [-0.7583,  0.0307],\n",
      "         [ 0.1332,  0.7561],\n",
      "         [-0.0606,  0.4415]]])\n",
      "Outputs dimensions: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = gpt(inputs)\n",
    "    print(\"Outputs:\\n\", outputs)\n",
    "    print(\"Outputs dimensions:\", outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6194714b",
   "metadata": {},
   "source": [
    "The first dimension `1` is the batch size, `4` is the token dimension, `2` is the output dimension.\n",
    "\n",
    "We use the output of the latest token as it is the prediction of the model when it uses all of the tokens as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f34b21c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-0.0606,  0.4415]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ae4515",
   "metadata": {},
   "source": [
    "In order to obtain the class label, we take the argmax. \n",
    "\n",
    "There is no need to apply the softmax as we don't need to know the probability scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a107171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "label = torch.argmax(probas)\n",
    "\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2baf00f",
   "metadata": {},
   "source": [
    "##### TODO: Exercise 6.3 Fine-tuning the first vs. last token\n",
    "Try fine-tuning the first output token. Notice the changes in predictive performance\n",
    "compared to fine-tuning the last output token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f162372d",
   "metadata": {},
   "source": [
    "##### Create function to measure loader's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e45efffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None): \n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i >= num_batches: break\n",
    "\n",
    "        input_batch = input_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_batch)[ :, -1, :]\n",
    "        predicted_labels = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        num_examples += predicted_labels.shape[0]\n",
    "        correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f884f86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 37.50%\n",
      "Val. Accuracy : 50.00%\n",
      "Test Accuracy : 37.50%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"mps\"\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(train_loader, gpt, device, 1)\n",
    "val_accuracy =  calc_accuracy_loader(val_loader, gpt, device, 1)\n",
    "test_accuracy =  calc_accuracy_loader(test_loader, gpt, device, 1)\n",
    "\n",
    "print (f\"Training Accuracy : {train_accuracy*100:.2f}%\" )\n",
    "print (f\"Val. Accuracy : {val_accuracy*100:.2f}%\" )\n",
    "print (f\"Test Accuracy : {test_accuracy*100:.2f}%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2122734",
   "metadata": {},
   "source": [
    "We need to implement a loss, to be able to train the classifier.\n",
    "Accuracy is not differentiable, so we use `cross_entropy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd28cab",
   "metadata": {},
   "source": [
    "##### Calculate Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f9dc8f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "\n",
    "    logits = model(input_batch)[:,-1,:]\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ceede36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i >= num_batches: break\n",
    "\n",
    "        input_batch = input_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            total_loss += calc_loss_batch(input_batch, target_batch, model, device)\n",
    "\n",
    "    return total_loss / num_batches # Average loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d48fe05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 11.444\n",
      "Validation loss: 11.320\n",
      "Test loss: 11.348\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(\n",
    "        train_loader, model, device, num_batches=5\n",
    "    )\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
    "\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
